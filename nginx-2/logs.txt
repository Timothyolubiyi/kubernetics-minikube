
==> Audit <==
|---------|---------------|----------|-----------------------|---------|---------------------|---------------------|
| Command |     Args      | Profile  |         User          | Version |     Start Time      |      End Time       |
|---------|---------------|----------|-----------------------|---------|---------------------|---------------------|
| start   |               | minikube | DESKTOP-JTVLLNC\timot | v1.36.0 | 27 Jul 25 14:27 WAT |                     |
| start   |               | minikube | DESKTOP-JTVLLNC\timot | v1.36.0 | 27 Jul 25 14:43 WAT |                     |
| start   |               | minikube | DESKTOP-JTVLLNC\timot | v1.36.0 | 27 Jul 25 14:45 WAT |                     |
| delete  |               | minikube | DESKTOP-JTVLLNC\timot | v1.36.0 | 27 Jul 25 14:47 WAT | 27 Jul 25 14:47 WAT |
| start   |               | minikube | DESKTOP-JTVLLNC\timot | v1.36.0 | 27 Jul 25 14:47 WAT | 27 Jul 25 14:50 WAT |
| start   |               | minikube | DESKTOP-JTVLLNC\timot | v1.36.0 | 29 Jul 25 22:31 WAT | 29 Jul 25 22:32 WAT |
| service | nginx -n test | minikube | DESKTOP-JTVLLNC\timot | v1.36.0 | 29 Jul 25 22:54 WAT | 29 Jul 25 22:56 WAT |
| service | nginx -n test | minikube | DESKTOP-JTVLLNC\timot | v1.36.0 | 29 Jul 25 22:59 WAT | 29 Jul 25 23:00 WAT |
| stop    |               | minikube | DESKTOP-JTVLLNC\timot | v1.36.0 | 29 Jul 25 23:06 WAT | 29 Jul 25 23:06 WAT |
| start   |               | minikube | DESKTOP-JTVLLNC\timot | v1.36.0 | 02 Aug 25 20:11 WAT | 02 Aug 25 20:12 WAT |
| service | nginx -n dev  | minikube | DESKTOP-JTVLLNC\timot | v1.36.0 | 02 Aug 25 20:46 WAT |                     |
| service | nginx -n dev  | minikube | DESKTOP-JTVLLNC\timot | v1.36.0 | 02 Aug 25 20:53 WAT |                     |
|---------|---------------|----------|-----------------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/08/02 20:11:00
Running on machine: DESKTOP-JTVLLNC
Binary: Built with gc go1.24.0 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0802 20:11:00.459786   10452 out.go:345] Setting OutFile to fd 508 ...
I0802 20:11:00.462607   10452 out.go:397] isatty.IsTerminal(508) = true
I0802 20:11:00.463234   10452 out.go:358] Setting ErrFile to fd 508...
I0802 20:11:00.463234   10452 out.go:397] isatty.IsTerminal(508) = true
I0802 20:11:00.508007   10452 out.go:352] Setting JSON to false
I0802 20:11:00.519760   10452 start.go:130] hostinfo: {"hostname":"DESKTOP-JTVLLNC","uptime":28037,"bootTime":1754133823,"procs":298,"os":"windows","platform":"Microsoft Windows 11 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.26100.4652 Build 26100.4652","kernelVersion":"10.0.26100.4652 Build 26100.4652","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"69002458-280e-4d4c-9b62-8667bfb9189d"}
W0802 20:11:00.520411   10452 start.go:138] gopshost.Virtualization returned error: not implemented yet
I0802 20:11:00.524075   10452 out.go:177] ðŸ˜„  minikube v1.36.0 on Microsoft Windows 11 Pro 10.0.26100.4652 Build 26100.4652
I0802 20:11:00.528664   10452 notify.go:220] Checking for updates...
I0802 20:11:00.530199   10452 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0802 20:11:00.534788   10452 driver.go:404] Setting default libvirt URI to qemu:///system
I0802 20:11:00.765364   10452 docker.go:123] docker version: linux-28.3.2:Docker Desktop 4.43.2 (199162)
I0802 20:11:00.782801   10452 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0802 20:11:01.826332   10452 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (1.043531s)
I0802 20:11:01.833662   10452 info.go:266] docker info: {ID:e2d1d825-0895-4304-af1d-0ca6a10b7c34 Containers:5 ContainersRunning:3 ContainersPaused:0 ContainersStopped:2 Images:8 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:82 OomKillDisable:false NGoroutines:108 SystemTime:2025-08-02 19:11:02.614598802 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:15 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:3974328320 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.3.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:[WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Users\timot\.docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-ai.exe] ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.9.9] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.25.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.4.2] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.38.2-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.41] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.1.11] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.29] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Program Files\Docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.9.9] map[Name:model Path:C:\Program Files\Docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner (EXPERIMENTAL) Vendor:Docker Inc. Version:v0.1.33] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Users\timot\.docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-scout.exe] ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.2]] Warnings:<nil>}}
I0802 20:11:01.839205   10452 out.go:177] âœ¨  Using the docker driver based on existing profile
I0802 20:11:01.842678   10452 start.go:304] selected driver: docker
I0802 20:11:01.842678   10452 start.go:908] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\timot:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0802 20:11:01.842678   10452 start.go:919] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0802 20:11:01.871530   10452 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0802 20:11:02.410315   10452 info.go:266] docker info: {ID:e2d1d825-0895-4304-af1d-0ca6a10b7c34 Containers:5 ContainersRunning:3 ContainersPaused:0 ContainersStopped:2 Images:8 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:82 OomKillDisable:false NGoroutines:108 SystemTime:2025-08-02 19:11:03.208858756 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:15 KernelVersion:6.6.87.2-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:3974328320 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.3.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:[WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Users\timot\.docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-ai.exe] ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.9.9] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.25.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.4.2] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.38.2-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.41] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.1.11] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.29] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Program Files\Docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.9.9] map[Name:model Path:C:\Program Files\Docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner (EXPERIMENTAL) Vendor:Docker Inc. Version:v0.1.33] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Users\timot\.docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-scout.exe] ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.2]] Warnings:<nil>}}
I0802 20:11:02.736761   10452 cni.go:84] Creating CNI manager for ""
I0802 20:11:02.737401   10452 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0802 20:11:02.738035   10452 start.go:347] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\timot:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0802 20:11:02.740897   10452 out.go:177] ðŸ‘  Starting "minikube" primary control-plane node in "minikube" cluster
I0802 20:11:02.747161   10452 cache.go:121] Beginning downloading kic base image for docker with docker
I0802 20:11:02.748894   10452 out.go:177] ðŸšœ  Pulling base image v0.0.47 ...
I0802 20:11:02.756433   10452 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0802 20:11:02.756433   10452 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local docker daemon
I0802 20:11:02.758976   10452 preload.go:146] Found local preload: C:\Users\timot\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4
I0802 20:11:02.758976   10452 cache.go:56] Caching tarball of preloaded images
I0802 20:11:02.758976   10452 preload.go:172] Found C:\Users\timot\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0802 20:11:02.758976   10452 cache.go:59] Finished verifying existence of preloaded tar for v1.33.1 on docker
I0802 20:11:02.758976   10452 profile.go:143] Saving config to C:\Users\timot\.minikube\profiles\minikube\config.json ...
I0802 20:11:03.109485   10452 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b to local cache
I0802 20:11:03.120455   10452 localpath.go:146] windows sanitize: C:\Users\timot\.minikube\cache\kic\amd64\kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar -> C:\Users\timot\.minikube\cache\kic\amd64\kicbase_v0.0.47@sha256_6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar
I0802 20:11:03.120455   10452 localpath.go:146] windows sanitize: C:\Users\timot\.minikube\cache\kic\amd64\kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar -> C:\Users\timot\.minikube\cache\kic\amd64\kicbase_v0.0.47@sha256_6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar
I0802 20:11:03.121167   10452 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local cache directory
I0802 20:11:03.125072   10452 image.go:68] Found gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local cache directory, skipping pull
I0802 20:11:03.125072   10452 image.go:137] gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b exists in cache, skipping pull
I0802 20:11:03.125690   10452 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b as a tarball
I0802 20:11:03.125690   10452 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b from local cache
I0802 20:11:03.125690   10452 localpath.go:146] windows sanitize: C:\Users\timot\.minikube\cache\kic\amd64\kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar -> C:\Users\timot\.minikube\cache\kic\amd64\kicbase_v0.0.47@sha256_6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar
I0802 20:11:47.407466   10452 cache.go:165] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b from cached tarball
I0802 20:11:47.408037   10452 cache.go:230] Successfully downloaded all kic artifacts
I0802 20:11:47.411368   10452 start.go:360] acquireMachinesLock for minikube: {Name:mk5edeedf7decaa050b499c213d9d9617bf8e14d Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0802 20:11:47.411901   10452 start.go:364] duration metric: took 532.3Âµs to acquireMachinesLock for "minikube"
I0802 20:11:47.414068   10452 start.go:96] Skipping create...Using existing machine configuration
I0802 20:11:47.414068   10452 fix.go:54] fixHost starting: 
I0802 20:11:47.456878   10452 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0802 20:11:47.614432   10452 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0802 20:11:47.614960   10452 fix.go:138] unexpected machine state, will restart: <nil>
I0802 20:11:47.617210   10452 out.go:177] ðŸ”„  Restarting existing docker container for "minikube" ...
I0802 20:11:47.635031   10452 cli_runner.go:164] Run: docker start minikube
I0802 20:11:48.763642   10452 cli_runner.go:217] Completed: docker start minikube: (1.1286114s)
I0802 20:11:48.781415   10452 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0802 20:11:48.929346   10452 kic.go:430] container "minikube" state is running.
I0802 20:11:48.950603   10452 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0802 20:11:49.082155   10452 profile.go:143] Saving config to C:\Users\timot\.minikube\profiles\minikube\config.json ...
I0802 20:11:49.086663   10452 machine.go:93] provisionDockerMachine start ...
I0802 20:11:49.104446   10452 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0802 20:11:49.317285   10452 main.go:141] libmachine: Using SSH client type: native
I0802 20:11:49.321360   10452 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xd5a9e0] 0xd5d520 <nil>  [] 0s} 127.0.0.1 56511 <nil> <nil>}
I0802 20:11:49.321360   10452 main.go:141] libmachine: About to run SSH command:
hostname
I0802 20:11:49.329807   10452 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0802 20:11:52.544532   10452 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0802 20:11:52.545087   10452 ubuntu.go:169] provisioning hostname "minikube"
I0802 20:11:52.555899   10452 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0802 20:11:52.670225   10452 main.go:141] libmachine: Using SSH client type: native
I0802 20:11:52.670776   10452 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xd5a9e0] 0xd5d520 <nil>  [] 0s} 127.0.0.1 56511 <nil> <nil>}
I0802 20:11:52.670776   10452 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0802 20:11:52.975463   10452 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0802 20:11:52.999559   10452 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0802 20:11:53.154241   10452 main.go:141] libmachine: Using SSH client type: native
I0802 20:11:53.154836   10452 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xd5a9e0] 0xd5d520 <nil>  [] 0s} 127.0.0.1 56511 <nil> <nil>}
I0802 20:11:53.154836   10452 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0802 20:11:53.381525   10452 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0802 20:11:53.383228   10452 ubuntu.go:175] set auth options {CertDir:C:\Users\timot\.minikube CaCertPath:C:\Users\timot\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\timot\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\timot\.minikube\machines\server.pem ServerKeyPath:C:\Users\timot\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\timot\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\timot\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\timot\.minikube}
I0802 20:11:53.383888   10452 ubuntu.go:177] setting up certificates
I0802 20:11:53.386903   10452 provision.go:84] configureAuth start
I0802 20:11:53.423411   10452 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0802 20:11:53.513202   10452 provision.go:143] copyHostCerts
I0802 20:11:53.530607   10452 exec_runner.go:144] found C:\Users\timot\.minikube/ca.pem, removing ...
I0802 20:11:53.531687   10452 exec_runner.go:203] rm: C:\Users\timot\.minikube\ca.pem
I0802 20:11:53.532235   10452 exec_runner.go:151] cp: C:\Users\timot\.minikube\certs\ca.pem --> C:\Users\timot\.minikube/ca.pem (1074 bytes)
I0802 20:11:53.537727   10452 exec_runner.go:144] found C:\Users\timot\.minikube/cert.pem, removing ...
I0802 20:11:53.537727   10452 exec_runner.go:203] rm: C:\Users\timot\.minikube\cert.pem
I0802 20:11:53.538297   10452 exec_runner.go:151] cp: C:\Users\timot\.minikube\certs\cert.pem --> C:\Users\timot\.minikube/cert.pem (1119 bytes)
I0802 20:11:53.541085   10452 exec_runner.go:144] found C:\Users\timot\.minikube/key.pem, removing ...
I0802 20:11:53.541085   10452 exec_runner.go:203] rm: C:\Users\timot\.minikube\key.pem
I0802 20:11:53.541644   10452 exec_runner.go:151] cp: C:\Users\timot\.minikube\certs\key.pem --> C:\Users\timot\.minikube/key.pem (1675 bytes)
I0802 20:11:53.542782   10452 provision.go:117] generating server cert: C:\Users\timot\.minikube\machines\server.pem ca-key=C:\Users\timot\.minikube\certs\ca.pem private-key=C:\Users\timot\.minikube\certs\ca-key.pem org=timot.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0802 20:11:55.534983   10452 provision.go:177] copyRemoteCerts
I0802 20:11:55.573202   10452 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0802 20:11:55.588838   10452 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0802 20:11:55.676714   10452 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:56511 SSHKeyPath:C:\Users\timot\.minikube\machines\minikube\id_rsa Username:docker}
I0802 20:11:55.800523   10452 ssh_runner.go:362] scp C:\Users\timot\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0802 20:11:55.884454   10452 ssh_runner.go:362] scp C:\Users\timot\.minikube\machines\server.pem --> /etc/docker/server.pem (1176 bytes)
I0802 20:11:55.973595   10452 ssh_runner.go:362] scp C:\Users\timot\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0802 20:11:56.043976   10452 provision.go:87] duration metric: took 2.6565576s to configureAuth
I0802 20:11:56.043976   10452 ubuntu.go:193] setting minikube options for container-runtime
I0802 20:11:56.044563   10452 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0802 20:11:56.056604   10452 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0802 20:11:56.124587   10452 main.go:141] libmachine: Using SSH client type: native
I0802 20:11:56.124587   10452 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xd5a9e0] 0xd5d520 <nil>  [] 0s} 127.0.0.1 56511 <nil> <nil>}
I0802 20:11:56.124587   10452 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0802 20:11:56.288380   10452 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0802 20:11:56.288380   10452 ubuntu.go:71] root file system type: overlay
I0802 20:11:56.288922   10452 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0802 20:11:56.297618   10452 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0802 20:11:56.359688   10452 main.go:141] libmachine: Using SSH client type: native
I0802 20:11:56.360350   10452 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xd5a9e0] 0xd5d520 <nil>  [] 0s} 127.0.0.1 56511 <nil> <nil>}
I0802 20:11:56.360350   10452 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0802 20:11:56.604401   10452 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0802 20:11:56.645524   10452 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0802 20:11:56.719855   10452 main.go:141] libmachine: Using SSH client type: native
I0802 20:11:56.720393   10452 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xd5a9e0] 0xd5d520 <nil>  [] 0s} 127.0.0.1 56511 <nil> <nil>}
I0802 20:11:56.720393   10452 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0802 20:11:56.890239   10452 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0802 20:11:56.890239   10452 machine.go:96] duration metric: took 7.8035764s to provisionDockerMachine
I0802 20:11:56.890784   10452 start.go:293] postStartSetup for "minikube" (driver="docker")
I0802 20:11:56.890784   10452 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0802 20:11:56.896999   10452 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0802 20:11:56.912516   10452 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0802 20:11:57.041441   10452 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:56511 SSHKeyPath:C:\Users\timot\.minikube\machines\minikube\id_rsa Username:docker}
I0802 20:11:57.218018   10452 ssh_runner.go:195] Run: cat /etc/os-release
I0802 20:11:57.231587   10452 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0802 20:11:57.231587   10452 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0802 20:11:57.231587   10452 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0802 20:11:57.231587   10452 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0802 20:11:57.231587   10452 filesync.go:126] Scanning C:\Users\timot\.minikube\addons for local assets ...
I0802 20:11:57.232846   10452 filesync.go:126] Scanning C:\Users\timot\.minikube\files for local assets ...
I0802 20:11:57.233489   10452 start.go:296] duration metric: took 342.7056ms for postStartSetup
I0802 20:11:57.341282   10452 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0802 20:11:57.357061   10452 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0802 20:11:57.491133   10452 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:56511 SSHKeyPath:C:\Users\timot\.minikube\machines\minikube\id_rsa Username:docker}
I0802 20:11:57.677344   10452 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0802 20:11:57.694303   10452 fix.go:56] duration metric: took 10.2794918s for fixHost
I0802 20:11:57.694303   10452 start.go:83] releasing machines lock for "minikube", held for 10.2824022s
I0802 20:11:57.710939   10452 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0802 20:11:57.846926   10452 ssh_runner.go:195] Run: cat /version.json
I0802 20:11:57.863344   10452 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0802 20:11:57.865090   10452 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I0802 20:11:57.885420   10452 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0802 20:11:58.030525   10452 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:56511 SSHKeyPath:C:\Users\timot\.minikube\machines\minikube\id_rsa Username:docker}
I0802 20:11:58.035234   10452 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:56511 SSHKeyPath:C:\Users\timot\.minikube\machines\minikube\id_rsa Username:docker}
W0802 20:11:58.145172   10452 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I0802 20:11:58.189403   10452 ssh_runner.go:195] Run: systemctl --version
I0802 20:11:58.218452   10452 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0802 20:11:58.244094   10452 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0802 20:11:58.280234   10452 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0802 20:11:58.287718   10452 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0802 20:11:58.319380   10452 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0802 20:11:58.319380   10452 start.go:495] detecting cgroup driver to use...
I0802 20:11:58.319380   10452 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0802 20:11:58.321767   10452 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0802 20:11:58.379107   10452 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0802 20:11:58.418514   10452 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0802 20:11:58.452023   10452 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0802 20:11:58.455602   10452 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0802 20:11:58.492862   10452 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0802 20:11:58.529397   10452 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0802 20:11:58.566082   10452 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0802 20:11:58.604332   10452 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0802 20:11:58.640133   10452 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0802 20:11:58.685031   10452 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0802 20:11:58.736198   10452 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0802 20:11:58.765858   10452 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0802 20:11:58.786452   10452 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0802 20:11:58.809884   10452 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0802 20:11:58.994057   10452 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0802 20:11:59.188121   10452 start.go:495] detecting cgroup driver to use...
I0802 20:11:59.188121   10452 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0802 20:11:59.194344   10452 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0802 20:11:59.226017   10452 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0802 20:11:59.231996   10452 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0802 20:11:59.258547   10452 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0802 20:11:59.309643   10452 ssh_runner.go:195] Run: which cri-dockerd
I0802 20:11:59.322844   10452 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0802 20:11:59.337437   10452 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0802 20:11:59.368142   10452 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0802 20:11:59.594677   10452 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0802 20:11:59.803256   10452 docker.go:587] configuring docker to use "cgroupfs" as cgroup driver...
I0802 20:11:59.804361   10452 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0802 20:11:59.867451   10452 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I0802 20:11:59.917979   10452 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0802 20:12:00.076821   10452 ssh_runner.go:195] Run: sudo systemctl restart docker
W0802 20:12:00.897285   10452 out.go:270] â—  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W0802 20:12:00.897966   10452 out.go:270] ðŸ’¡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0802 20:12:04.258281   10452 ssh_runner.go:235] Completed: sudo systemctl restart docker: (4.1814595s)
I0802 20:12:04.265376   10452 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0802 20:12:04.309877   10452 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0802 20:12:04.359598   10452 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0802 20:12:04.403089   10452 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0802 20:12:04.642320   10452 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0802 20:12:04.843571   10452 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0802 20:12:04.976366   10452 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0802 20:12:05.048030   10452 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I0802 20:12:05.089265   10452 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0802 20:12:05.294040   10452 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0802 20:12:06.030165   10452 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0802 20:12:06.051009   10452 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0802 20:12:06.058596   10452 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0802 20:12:06.068985   10452 start.go:563] Will wait 60s for crictl version
I0802 20:12:06.070639   10452 ssh_runner.go:195] Run: which crictl
I0802 20:12:06.083118   10452 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0802 20:12:06.303607   10452 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.1.1
RuntimeApiVersion:  v1
I0802 20:12:06.314345   10452 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0802 20:12:06.581317   10452 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0802 20:12:06.666179   10452 out.go:235] ðŸ³  Preparing Kubernetes v1.33.1 on Docker 28.1.1 ...
I0802 20:12:06.686984   10452 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0802 20:12:07.053381   10452 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0802 20:12:07.067436   10452 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0802 20:12:07.079949   10452 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0802 20:12:07.127671   10452 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0802 20:12:07.246690   10452 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\timot:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0802 20:12:07.247240   10452 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0802 20:12:07.263631   10452 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0802 20:12:07.332441   10452 docker.go:702] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/pause:3.10
kicbase/echo-server:1.0
gcr.io/k8s-minikube/storage-provisioner:v5
nginx:1.14.2

-- /stdout --
I0802 20:12:07.332441   10452 docker.go:632] Images already preloaded, skipping extraction
I0802 20:12:07.351187   10452 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0802 20:12:07.412149   10452 docker.go:702] Got preloaded images: -- stdout --
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/pause:3.10
kicbase/echo-server:1.0
gcr.io/k8s-minikube/storage-provisioner:v5
nginx:1.14.2

-- /stdout --
I0802 20:12:07.412737   10452 cache_images.go:84] Images are preloaded, skipping loading
I0802 20:12:07.412737   10452 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.33.1 docker true true} ...
I0802 20:12:07.414499   10452 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.33.1/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0802 20:12:07.437768   10452 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0802 20:12:08.029923   10452 cni.go:84] Creating CNI manager for ""
I0802 20:12:08.029923   10452 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0802 20:12:08.031213   10452 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0802 20:12:08.031213   10452 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.33.1 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0802 20:12:08.031213   10452 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.33.1
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0802 20:12:08.040208   10452 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.33.1
I0802 20:12:08.074071   10452 binaries.go:44] Found k8s binaries, skipping transfer
I0802 20:12:08.082654   10452 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0802 20:12:08.111980   10452 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0802 20:12:08.170591   10452 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0802 20:12:08.242378   10452 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0802 20:12:08.292004   10452 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0802 20:12:08.303233   10452 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0802 20:12:08.343985   10452 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0802 20:12:08.456725   10452 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0802 20:12:08.510200   10452 certs.go:68] Setting up C:\Users\timot\.minikube\profiles\minikube for IP: 192.168.49.2
I0802 20:12:08.510745   10452 certs.go:194] generating shared ca certs ...
I0802 20:12:08.511299   10452 certs.go:226] acquiring lock for ca certs: {Name:mk144c6297445844f5688a757515871f99505e32 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0802 20:12:08.514259   10452 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\timot\.minikube\ca.key
I0802 20:12:08.516478   10452 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\timot\.minikube\proxy-client-ca.key
I0802 20:12:08.516478   10452 certs.go:256] generating profile certs ...
I0802 20:12:08.517579   10452 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\timot\.minikube\profiles\minikube\client.key
I0802 20:12:08.520383   10452 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\timot\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0802 20:12:08.523392   10452 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\timot\.minikube\profiles\minikube\proxy-client.key
I0802 20:12:08.529502   10452 certs.go:484] found cert: C:\Users\timot\.minikube\certs\ca-key.pem (1675 bytes)
I0802 20:12:08.530074   10452 certs.go:484] found cert: C:\Users\timot\.minikube\certs\ca.pem (1074 bytes)
I0802 20:12:08.530664   10452 certs.go:484] found cert: C:\Users\timot\.minikube\certs\cert.pem (1119 bytes)
I0802 20:12:08.530664   10452 certs.go:484] found cert: C:\Users\timot\.minikube\certs\key.pem (1675 bytes)
I0802 20:12:08.540600   10452 ssh_runner.go:362] scp C:\Users\timot\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0802 20:12:08.638357   10452 ssh_runner.go:362] scp C:\Users\timot\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0802 20:12:08.694987   10452 ssh_runner.go:362] scp C:\Users\timot\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0802 20:12:08.782440   10452 ssh_runner.go:362] scp C:\Users\timot\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0802 20:12:08.850551   10452 ssh_runner.go:362] scp C:\Users\timot\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0802 20:12:08.938653   10452 ssh_runner.go:362] scp C:\Users\timot\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0802 20:12:09.012393   10452 ssh_runner.go:362] scp C:\Users\timot\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0802 20:12:09.091344   10452 ssh_runner.go:362] scp C:\Users\timot\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0802 20:12:09.147120   10452 ssh_runner.go:362] scp C:\Users\timot\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0802 20:12:09.379328   10452 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0802 20:12:09.655483   10452 ssh_runner.go:195] Run: openssl version
I0802 20:12:09.719086   10452 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0802 20:12:09.815753   10452 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0802 20:12:09.830049   10452 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Jul 27 13:50 /usr/share/ca-certificates/minikubeCA.pem
I0802 20:12:09.831712   10452 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0802 20:12:09.912172   10452 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0802 20:12:10.014477   10452 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0802 20:12:10.103614   10452 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0802 20:12:10.223777   10452 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0802 20:12:10.322330   10452 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0802 20:12:10.425825   10452 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0802 20:12:10.504569   10452 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0802 20:12:10.545813   10452 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0802 20:12:10.624090   10452 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\timot:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0802 20:12:10.657787   10452 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0802 20:12:11.145133   10452 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0802 20:12:11.425504   10452 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0802 20:12:11.426092   10452 kubeadm.go:589] restartPrimaryControlPlane start ...
I0802 20:12:11.436553   10452 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0802 20:12:11.646878   10452 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0802 20:12:11.667014   10452 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0802 20:12:11.848750   10452 kubeconfig.go:47] verify endpoint returned: get endpoint: "minikube" does not appear in C:\Users\timot\.kube\config
I0802 20:12:11.851020   10452 kubeconfig.go:62] C:\Users\timot\.kube\config needs updating (will repair): [kubeconfig missing "minikube" cluster setting kubeconfig missing "minikube" context setting]
I0802 20:12:11.858664   10452 lock.go:35] WriteFile acquiring C:\Users\timot\.kube\config: {Name:mk0c9c0f028ef30f87326cf571ed72ec8205508e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0802 20:12:11.896026   10452 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0802 20:12:12.060869   10452 kubeadm.go:626] The running cluster does not require reconfiguration: 127.0.0.1
I0802 20:12:12.060869   10452 kubeadm.go:593] duration metric: took 634.7763ms to restartPrimaryControlPlane
I0802 20:12:12.060869   10452 kubeadm.go:394] duration metric: took 1.4367784s to StartCluster
I0802 20:12:12.060869   10452 settings.go:142] acquiring lock: {Name:mkd47b3e8abe48a00d4b75402f37b0f801e432bb Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0802 20:12:12.061502   10452 settings.go:150] Updating kubeconfig:  C:\Users\timot\.kube\config
I0802 20:12:12.065357   10452 lock.go:35] WriteFile acquiring C:\Users\timot\.kube\config: {Name:mk0c9c0f028ef30f87326cf571ed72ec8205508e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0802 20:12:12.068560   10452 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0802 20:12:12.068560   10452 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0802 20:12:12.069193   10452 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0802 20:12:12.071488   10452 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0802 20:12:12.071488   10452 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0802 20:12:12.073656   10452 out.go:177] ðŸ”Ž  Verifying Kubernetes components...
I0802 20:12:12.073656   10452 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0802 20:12:12.073656   10452 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0802 20:12:12.073656   10452 addons.go:247] addon storage-provisioner should already be in state true
I0802 20:12:12.075540   10452 host.go:66] Checking if "minikube" exists ...
I0802 20:12:12.087020   10452 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0802 20:12:12.120297   10452 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0802 20:12:12.123934   10452 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0802 20:12:12.256873   10452 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0802 20:12:12.256873   10452 addons.go:247] addon default-storageclass should already be in state true
I0802 20:12:12.258126   10452 host.go:66] Checking if "minikube" exists ...
I0802 20:12:12.269303   10452 out.go:177]     â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0802 20:12:12.274035   10452 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0802 20:12:12.274035   10452 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0802 20:12:12.282742   10452 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0802 20:12:12.285819   10452 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0802 20:12:12.389393   10452 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:56511 SSHKeyPath:C:\Users\timot\.minikube\machines\minikube\id_rsa Username:docker}
I0802 20:12:12.395705   10452 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0802 20:12:12.395705   10452 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0802 20:12:12.411945   10452 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0802 20:12:12.548198   10452 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:56511 SSHKeyPath:C:\Users\timot\.minikube\machines\minikube\id_rsa Username:docker}
I0802 20:12:12.669997   10452 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0802 20:12:12.886220   10452 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0802 20:12:12.908208   10452 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0802 20:12:12.998087   10452 api_server.go:52] waiting for apiserver process to appear ...
I0802 20:12:13.002492   10452 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0802 20:12:13.198147   10452 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0802 20:12:16.083867   10452 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (3.1756585s)
I0802 20:12:16.084468   10452 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (3.0813747s)
W0802 20:12:16.084468   10452 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0802 20:12:16.084468   10452 retry.go:31] will retry after 364.525663ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0802 20:12:16.091039   10452 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0802 20:12:16.153878   10452 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (2.9557313s)
W0802 20:12:16.153878   10452 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0802 20:12:16.153878   10452 retry.go:31] will retry after 185.650345ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0802 20:12:16.254986   10452 api_server.go:72] duration metric: took 4.1864259s to wait for apiserver process to appear ...
I0802 20:12:16.255493   10452 api_server.go:88] waiting for apiserver healthz status ...
I0802 20:12:16.256034   10452 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:56508/healthz ...
I0802 20:12:16.261327   10452 api_server.go:269] stopped: https://127.0.0.1:56508/healthz: Get "https://127.0.0.1:56508/healthz": EOF
I0802 20:12:16.345435   10452 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0802 20:12:16.457316   10452 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0802 20:12:16.755898   10452 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:56508/healthz ...
I0802 20:12:16.758138   10452 api_server.go:269] stopped: https://127.0.0.1:56508/healthz: Get "https://127.0.0.1:56508/healthz": EOF
I0802 20:12:17.255836   10452 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:56508/healthz ...
I0802 20:12:17.260305   10452 api_server.go:269] stopped: https://127.0.0.1:56508/healthz: Get "https://127.0.0.1:56508/healthz": EOF
I0802 20:12:17.756208   10452 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:56508/healthz ...
I0802 20:12:17.760849   10452 api_server.go:269] stopped: https://127.0.0.1:56508/healthz: Get "https://127.0.0.1:56508/healthz": EOF
I0802 20:12:17.926031   10452 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (1.5805961s)
W0802 20:12:17.926031   10452 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0802 20:12:17.926031   10452 retry.go:31] will retry after 316.421827ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0802 20:12:18.035230   10452 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.5779137s)
W0802 20:12:18.035230   10452 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0802 20:12:18.035925   10452 retry.go:31] will retry after 414.511063ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0802 20:12:18.248563   10452 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0802 20:12:18.256008   10452 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:56508/healthz ...
I0802 20:12:18.262922   10452 api_server.go:269] stopped: https://127.0.0.1:56508/healthz: Get "https://127.0.0.1:56508/healthz": EOF
I0802 20:12:18.474852   10452 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0802 20:12:18.767973   10452 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:56508/healthz ...
I0802 20:12:18.781047   10452 api_server.go:269] stopped: https://127.0.0.1:56508/healthz: Get "https://127.0.0.1:56508/healthz": EOF
I0802 20:12:19.256587   10452 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:56508/healthz ...
I0802 20:12:19.260593   10452 api_server.go:269] stopped: https://127.0.0.1:56508/healthz: Get "https://127.0.0.1:56508/healthz": EOF
I0802 20:12:19.755931   10452 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:56508/healthz ...
I0802 20:12:19.760863   10452 api_server.go:269] stopped: https://127.0.0.1:56508/healthz: Get "https://127.0.0.1:56508/healthz": EOF
I0802 20:12:19.902527   10452 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (1.653964s)
W0802 20:12:19.902527   10452 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0802 20:12:19.902527   10452 retry.go:31] will retry after 462.73458ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0802 20:12:19.988013   10452 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.5131606s)
W0802 20:12:19.988013   10452 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0802 20:12:19.988013   10452 retry.go:31] will retry after 633.133421ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0802 20:12:20.256023   10452 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:56508/healthz ...
I0802 20:12:20.257885   10452 api_server.go:269] stopped: https://127.0.0.1:56508/healthz: Get "https://127.0.0.1:56508/healthz": EOF
I0802 20:12:20.371404   10452 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0802 20:12:20.632601   10452 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0802 20:12:20.756079   10452 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:56508/healthz ...
I0802 20:12:25.757244   10452 api_server.go:269] stopped: https://127.0.0.1:56508/healthz: Get "https://127.0.0.1:56508/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0802 20:12:25.757244   10452 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:56508/healthz ...
I0802 20:12:28.122033   10452 api_server.go:279] https://127.0.0.1:56508/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0802 20:12:28.122624   10452 api_server.go:103] status: https://127.0.0.1:56508/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0802 20:12:28.123159   10452 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:56508/healthz ...
I0802 20:12:28.238918   10452 api_server.go:279] https://127.0.0.1:56508/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0802 20:12:28.238918   10452 api_server.go:103] status: https://127.0.0.1:56508/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0802 20:12:28.256161   10452 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:56508/healthz ...
I0802 20:12:28.416994   10452 api_server.go:279] https://127.0.0.1:56508/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0802 20:12:28.416994   10452 api_server.go:103] status: https://127.0.0.1:56508/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0802 20:12:28.755850   10452 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:56508/healthz ...
I0802 20:12:28.935791   10452 api_server.go:279] https://127.0.0.1:56508/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0802 20:12:28.935791   10452 api_server.go:103] status: https://127.0.0.1:56508/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0802 20:12:29.255669   10452 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:56508/healthz ...
I0802 20:12:29.551065   10452 api_server.go:279] https://127.0.0.1:56508/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0802 20:12:29.551065   10452 api_server.go:103] status: https://127.0.0.1:56508/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0802 20:12:29.757555   10452 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:56508/healthz ...
I0802 20:12:29.863481   10452 api_server.go:279] https://127.0.0.1:56508/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0802 20:12:29.863481   10452 api_server.go:103] status: https://127.0.0.1:56508/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0802 20:12:30.159705   10452 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (9.7883001s)
I0802 20:12:30.256138   10452 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:56508/healthz ...
I0802 20:12:30.378182   10452 api_server.go:279] https://127.0.0.1:56508/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0802 20:12:30.378182   10452 api_server.go:103] status: https://127.0.0.1:56508/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0802 20:12:30.755846   10452 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:56508/healthz ...
I0802 20:12:30.797586   10452 api_server.go:279] https://127.0.0.1:56508/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0802 20:12:30.797586   10452 api_server.go:103] status: https://127.0.0.1:56508/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0802 20:12:31.256258   10452 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:56508/healthz ...
I0802 20:12:31.312161   10452 api_server.go:279] https://127.0.0.1:56508/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0802 20:12:31.312512   10452 api_server.go:103] status: https://127.0.0.1:56508/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0802 20:12:31.755713   10452 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:56508/healthz ...
I0802 20:12:31.909264   10452 api_server.go:279] https://127.0.0.1:56508/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0802 20:12:31.909264   10452 api_server.go:103] status: https://127.0.0.1:56508/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0802 20:12:32.256101   10452 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:56508/healthz ...
I0802 20:12:32.333024   10452 api_server.go:279] https://127.0.0.1:56508/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0802 20:12:32.333024   10452 api_server.go:103] status: https://127.0.0.1:56508/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0802 20:12:32.755994   10452 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:56508/healthz ...
I0802 20:12:32.845582   10452 api_server.go:279] https://127.0.0.1:56508/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0802 20:12:32.845582   10452 api_server.go:103] status: https://127.0.0.1:56508/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0802 20:12:33.256266   10452 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:56508/healthz ...
I0802 20:12:33.358991   10452 api_server.go:279] https://127.0.0.1:56508/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0802 20:12:33.358991   10452 api_server.go:103] status: https://127.0.0.1:56508/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0802 20:12:33.757693   10452 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:56508/healthz ...
I0802 20:12:33.785725   10452 api_server.go:279] https://127.0.0.1:56508/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0802 20:12:33.785725   10452 api_server.go:103] status: https://127.0.0.1:56508/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0802 20:12:34.256043   10452 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:56508/healthz ...
I0802 20:12:34.286269   10452 api_server.go:279] https://127.0.0.1:56508/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0802 20:12:34.286269   10452 api_server.go:103] status: https://127.0.0.1:56508/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0802 20:12:34.756011   10452 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:56508/healthz ...
I0802 20:12:34.799330   10452 api_server.go:279] https://127.0.0.1:56508/healthz returned 200:
ok
I0802 20:12:34.915728   10452 api_server.go:141] control plane version: v1.33.1
I0802 20:12:34.915728   10452 api_server.go:131] duration metric: took 18.6602343s to wait for apiserver health ...
I0802 20:12:34.918485   10452 system_pods.go:43] waiting for kube-system pods to appear ...
I0802 20:12:35.025440   10452 system_pods.go:59] 7 kube-system pods found
I0802 20:12:35.025979   10452 system_pods.go:61] "coredns-674b8bbfcf-mblwn" [14bd60e4-2a43-43c2-94f5-0c2ff8094219] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0802 20:12:35.025979   10452 system_pods.go:61] "etcd-minikube" [8f53afd2-d651-476e-8ee4-5cc493d96178] Running
I0802 20:12:35.025979   10452 system_pods.go:61] "kube-apiserver-minikube" [232ef1a2-4ae9-49ea-87c2-a7a3d8713c68] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0802 20:12:35.025979   10452 system_pods.go:61] "kube-controller-manager-minikube" [855c5a26-c818-48c0-8867-a077fd975652] Running
I0802 20:12:35.025979   10452 system_pods.go:61] "kube-proxy-p84w6" [f4f23e45-43db-4c8e-af91-3383849b9aff] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0802 20:12:35.025979   10452 system_pods.go:61] "kube-scheduler-minikube" [23776456-dc68-4cf5-951d-f914764a81de] Running
I0802 20:12:35.025979   10452 system_pods.go:61] "storage-provisioner" [54e91ec2-b11d-4501-a7d4-6465c0b74a54] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0802 20:12:35.025979   10452 system_pods.go:74] duration metric: took 107.4942ms to wait for pod list to return data ...
I0802 20:12:35.025979   10452 kubeadm.go:578] duration metric: took 22.9574185s to wait for: map[apiserver:true system_pods:true]
I0802 20:12:35.027302   10452 node_conditions.go:102] verifying NodePressure condition ...
I0802 20:12:35.109042   10452 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0802 20:12:35.112474   10452 node_conditions.go:123] node cpu capacity is 8
I0802 20:12:35.115394   10452 node_conditions.go:105] duration metric: took 88.0927ms to run NodePressure ...
I0802 20:12:35.115394   10452 start.go:241] waiting for startup goroutines ...
I0802 20:12:35.222023   10452 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (14.589422s)
I0802 20:12:35.230913   10452 out.go:177] ðŸŒŸ  Enabled addons: default-storageclass, storage-provisioner
I0802 20:12:35.236128   10452 addons.go:514] duration metric: took 23.1681256s for enable addons: enabled=[default-storageclass storage-provisioner]
I0802 20:12:35.236741   10452 start.go:246] waiting for cluster config update ...
I0802 20:12:35.236741   10452 start.go:255] writing updated cluster config ...
I0802 20:12:35.254493   10452 ssh_runner.go:195] Run: rm -f paused
I0802 20:12:35.641707   10452 start.go:607] kubectl: 1.32.2, cluster: 1.33.1 (minor skew: 1)
I0802 20:12:35.645062   10452 out.go:177] ðŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Aug 02 19:12:02 minikube dockerd[724]: time="2025-08-02T19:12:02.571767556Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Aug 02 19:12:02 minikube dockerd[724]: time="2025-08-02T19:12:02.572420398Z" level=info msg="Daemon shutdown complete"
Aug 02 19:12:02 minikube dockerd[724]: time="2025-08-02T19:12:02.572492461Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=plugins.moby
Aug 02 19:12:02 minikube systemd[1]: docker.service: Deactivated successfully.
Aug 02 19:12:02 minikube systemd[1]: Stopped Docker Application Container Engine.
Aug 02 19:12:02 minikube systemd[1]: docker.service: Consumed 1.506s CPU time.
Aug 02 19:12:02 minikube systemd[1]: Starting Docker Application Container Engine...
Aug 02 19:12:02 minikube dockerd[1061]: time="2025-08-02T19:12:02.662280061Z" level=info msg="Starting up"
Aug 02 19:12:02 minikube dockerd[1061]: time="2025-08-02T19:12:02.664245375Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Aug 02 19:12:02 minikube dockerd[1061]: time="2025-08-02T19:12:02.681205874Z" level=info msg="Creating a containerd client" address=/run/containerd/containerd.sock timeout=1m0s
Aug 02 19:12:02 minikube dockerd[1061]: time="2025-08-02T19:12:02.703622561Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Aug 02 19:12:02 minikube dockerd[1061]: time="2025-08-02T19:12:02.733920098Z" level=info msg="Loading containers: start."
Aug 02 19:12:04 minikube dockerd[1061]: time="2025-08-02T19:12:04.828248269Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint_count 01bfa955645c730508432c02d88f24dbf94504df971eb4b0eab678bc8acc746f], retrying...."
Aug 02 19:12:04 minikube dockerd[1061]: time="2025-08-02T19:12:04.943507653Z" level=warning msg="error locating sandbox id a4851e4d0dcd1fd05b02dd9716a7c6cc400cab218729d3c1221f196ffadfd2ea: sandbox a4851e4d0dcd1fd05b02dd9716a7c6cc400cab218729d3c1221f196ffadfd2ea not found"
Aug 02 19:12:04 minikube dockerd[1061]: time="2025-08-02T19:12:04.943668463Z" level=warning msg="error locating sandbox id dbc28a4f40c6bd35a65240c82800602cb5cf28bfca8f94265d95125248c83634: sandbox dbc28a4f40c6bd35a65240c82800602cb5cf28bfca8f94265d95125248c83634 not found"
Aug 02 19:12:04 minikube dockerd[1061]: time="2025-08-02T19:12:04.943707453Z" level=warning msg="error locating sandbox id 242fc88e6cc156506d9a544a57932f586ad907282d40f54d5bb83825adeb1fae: sandbox 242fc88e6cc156506d9a544a57932f586ad907282d40f54d5bb83825adeb1fae not found"
Aug 02 19:12:04 minikube dockerd[1061]: time="2025-08-02T19:12:04.943732865Z" level=warning msg="error locating sandbox id 54678ad1d4bb33bdb8f534494c25b9c9fee6d56c36bebf096a99d7866b07eb04: sandbox 54678ad1d4bb33bdb8f534494c25b9c9fee6d56c36bebf096a99d7866b07eb04 not found"
Aug 02 19:12:04 minikube dockerd[1061]: time="2025-08-02T19:12:04.943772049Z" level=warning msg="error locating sandbox id 15c5de50d0a9f3ab30366a03602ab2f73008cd81e31a150e70b2596ed442711f: sandbox 15c5de50d0a9f3ab30366a03602ab2f73008cd81e31a150e70b2596ed442711f not found"
Aug 02 19:12:04 minikube dockerd[1061]: time="2025-08-02T19:12:04.943793290Z" level=warning msg="error locating sandbox id c7e1fd21f723eda844bc008791a96484ba805e985f91992045bfbccdb63a1470: sandbox c7e1fd21f723eda844bc008791a96484ba805e985f91992045bfbccdb63a1470 not found"
Aug 02 19:12:04 minikube dockerd[1061]: time="2025-08-02T19:12:04.943814046Z" level=warning msg="error locating sandbox id 1c2253ab10f6108e50258eb88bef421b3d35f015863466b358e6ce74ba386f08: sandbox 1c2253ab10f6108e50258eb88bef421b3d35f015863466b358e6ce74ba386f08 not found"
Aug 02 19:12:04 minikube dockerd[1061]: time="2025-08-02T19:12:04.944557864Z" level=info msg="Loading containers: done."
Aug 02 19:12:04 minikube dockerd[1061]: time="2025-08-02T19:12:04.983011024Z" level=info msg="Docker daemon" commit=01f442b containerd-snapshotter=false storage-driver=overlay2 version=28.1.1
Aug 02 19:12:04 minikube dockerd[1061]: time="2025-08-02T19:12:04.983225178Z" level=info msg="Initializing buildkit"
Aug 02 19:12:05 minikube dockerd[1061]: time="2025-08-02T19:12:05.044908483Z" level=info msg="Completed buildkit initialization"
Aug 02 19:12:05 minikube dockerd[1061]: time="2025-08-02T19:12:05.060463204Z" level=info msg="Daemon has completed initialization"
Aug 02 19:12:05 minikube dockerd[1061]: time="2025-08-02T19:12:05.060793747Z" level=info msg="API listen on [::]:2376"
Aug 02 19:12:05 minikube dockerd[1061]: time="2025-08-02T19:12:05.060854269Z" level=info msg="API listen on /var/run/docker.sock"
Aug 02 19:12:05 minikube systemd[1]: Started Docker Application Container Engine.
Aug 02 19:12:06 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Aug 02 19:12:06 minikube cri-dockerd[1400]: time="2025-08-02T19:12:06Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Aug 02 19:12:06 minikube cri-dockerd[1400]: time="2025-08-02T19:12:06Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Aug 02 19:12:06 minikube cri-dockerd[1400]: time="2025-08-02T19:12:06Z" level=info msg="Start docker client with request timeout 0s"
Aug 02 19:12:06 minikube cri-dockerd[1400]: time="2025-08-02T19:12:06Z" level=info msg="Hairpin mode is set to hairpin-veth"
Aug 02 19:12:06 minikube cri-dockerd[1400]: time="2025-08-02T19:12:06Z" level=info msg="Loaded network plugin cni"
Aug 02 19:12:06 minikube cri-dockerd[1400]: time="2025-08-02T19:12:06Z" level=info msg="Docker cri networking managed by network plugin cni"
Aug 02 19:12:06 minikube cri-dockerd[1400]: time="2025-08-02T19:12:06Z" level=info msg="Setting cgroupDriver cgroupfs"
Aug 02 19:12:06 minikube cri-dockerd[1400]: time="2025-08-02T19:12:06Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Aug 02 19:12:06 minikube cri-dockerd[1400]: time="2025-08-02T19:12:06Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Aug 02 19:12:06 minikube cri-dockerd[1400]: time="2025-08-02T19:12:06Z" level=info msg="Start cri-dockerd grpc backend"
Aug 02 19:12:06 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Aug 02 19:12:10 minikube cri-dockerd[1400]: time="2025-08-02T19:12:10Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-674b8bbfcf-mblwn_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"3658be790bcd32ada1be202f8e6279a2e9a6bb8df770dc1a3818d3d2f87900d2\""
Aug 02 19:12:10 minikube cri-dockerd[1400]: time="2025-08-02T19:12:10Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-674b8bbfcf-mblwn_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"dcbab057422ea9a598870640cf05d6298944d3e40c4312e3c2761f41ff116c6d\""
Aug 02 19:12:10 minikube cri-dockerd[1400]: time="2025-08-02T19:12:10Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"hello-minikube-ffcbb5874-5gx2r_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"66195d1f02d6006e96b8f6a7f84c66cf54d78eed327d6f12342bb3e19a8bf5ff\""
Aug 02 19:12:10 minikube cri-dockerd[1400]: time="2025-08-02T19:12:10Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"hello-minikube-ffcbb5874-5gx2r_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"7dc06d97edde81fc576692b8150a7762e6b1114703e3e833be355220f6c19e7c\""
Aug 02 19:12:12 minikube cri-dockerd[1400]: time="2025-08-02T19:12:12Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"hello-minikube-ffcbb5874-5gx2r_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"66195d1f02d6006e96b8f6a7f84c66cf54d78eed327d6f12342bb3e19a8bf5ff\""
Aug 02 19:12:12 minikube cri-dockerd[1400]: time="2025-08-02T19:12:12Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"hello-minikube-ffcbb5874-5gx2r_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"7dc06d97edde81fc576692b8150a7762e6b1114703e3e833be355220f6c19e7c\""
Aug 02 19:12:12 minikube cri-dockerd[1400]: time="2025-08-02T19:12:12Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-674b8bbfcf-mblwn_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"3658be790bcd32ada1be202f8e6279a2e9a6bb8df770dc1a3818d3d2f87900d2\""
Aug 02 19:12:12 minikube cri-dockerd[1400]: time="2025-08-02T19:12:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9d69f2cb8a21db51f3ee364654c10462249ca96bd2b3edba776807672f19bd22/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 02 19:12:12 minikube cri-dockerd[1400]: time="2025-08-02T19:12:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/27a250589b3257b019013a55210c229e2a327bc660422bf7c676d02316bef1fc/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 02 19:12:13 minikube cri-dockerd[1400]: time="2025-08-02T19:12:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3028d26dba20b419f5ef66647c937799b2dcd934d1aa3d7fcec7c31ec86d0bcc/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 02 19:12:13 minikube cri-dockerd[1400]: time="2025-08-02T19:12:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/95cd7b6a55df87e128bc8c39b9615b25d9152fcc172f50696435f91452cb681e/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 02 19:12:15 minikube cri-dockerd[1400]: time="2025-08-02T19:12:15Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"hello-minikube-ffcbb5874-5gx2r_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"66195d1f02d6006e96b8f6a7f84c66cf54d78eed327d6f12342bb3e19a8bf5ff\""
Aug 02 19:12:28 minikube cri-dockerd[1400]: time="2025-08-02T19:12:28Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Aug 02 19:12:34 minikube cri-dockerd[1400]: time="2025-08-02T19:12:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/25893c59c0ad503fd4b1d9ad6aabb5ef19f7eccb4e64b889282d46c7be1e32d9/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 02 19:12:34 minikube cri-dockerd[1400]: time="2025-08-02T19:12:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1872b20842456ff792021d7ca5e2883d266f2f0029263a091270174cf03b118b/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 02 19:12:34 minikube cri-dockerd[1400]: time="2025-08-02T19:12:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1374c73cba991834a67f4959a1efaef97ce8191c9a17e04ae73743ecf82cd2ae/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 02 19:12:35 minikube cri-dockerd[1400]: time="2025-08-02T19:12:35Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/48f307c31c6e82122b51a339cefeabd1ad12242ebfc7feba6e5803fe5d71c3ee/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 02 19:12:57 minikube dockerd[1061]: time="2025-08-02T19:12:57.782716023Z" level=info msg="ignoring event" container=ed5cd255bd9f618c9389e7ebd1a8e8ae0bb3cc668063f8b1d5de269294fd558e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 02 19:40:36 minikube cri-dockerd[1400]: time="2025-08-02T19:40:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6b408f80bd764aecfb727b0f2dd5de50db428e250bbf21ff8ceaa561454cc98f/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 02 19:40:36 minikube cri-dockerd[1400]: time="2025-08-02T19:40:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/727a70a08d7be4da9528e453459ebb9e51962f384b7f2767f9918827c359074c/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
fd69f3d953360       295c7be079025       13 minutes ago      Running             nginx                     0                   727a70a08d7be       nginx-deployment-647677fc66-zb46f
54d831ee615da       295c7be079025       13 minutes ago      Running             nginx                     0                   6b408f80bd764       nginx-deployment-647677fc66-rdrp4
0ff83287cd50b       6e38f40d628db       40 minutes ago      Running             storage-provisioner       5                   1872b20842456       storage-provisioner
a5c57c462ddff       9056ab77afb8e       41 minutes ago      Running             echo-server               2                   1374c73cba991       hello-minikube-ffcbb5874-5gx2r
1613d9f98aa76       1cf5f116067c6       41 minutes ago      Running             coredns                   2                   48f307c31c6e8       coredns-674b8bbfcf-mblwn
f97a9be228b9f       b79c189b052cd       41 minutes ago      Running             kube-proxy                2                   25893c59c0ad5       kube-proxy-p84w6
ed5cd255bd9f6       6e38f40d628db       41 minutes ago      Exited              storage-provisioner       4                   1872b20842456       storage-provisioner
ac4627797f8ed       c6ab243b29f82       41 minutes ago      Running             kube-apiserver            2                   95cd7b6a55df8       kube-apiserver-minikube
95223a401b60a       499038711c081       41 minutes ago      Running             etcd                      2                   3028d26dba20b       etcd-minikube
2005ba0aefe55       398c985c0d950       41 minutes ago      Running             kube-scheduler            2                   9d69f2cb8a21d       kube-scheduler-minikube
5cc9688ddb823       ef43894fa110c       41 minutes ago      Running             kube-controller-manager   2                   27a250589b325       kube-controller-manager-minikube
70bc5c59c4139       1cf5f116067c6       3 days ago          Exited              coredns                   1                   3658be790bcd3       coredns-674b8bbfcf-mblwn
2892d9d06cc2b       9056ab77afb8e       3 days ago          Exited              echo-server               1                   66195d1f02d60       hello-minikube-ffcbb5874-5gx2r
0c973f6e730ff       b79c189b052cd       3 days ago          Exited              kube-proxy                1                   5f4b0ab1372ed       kube-proxy-p84w6
902f2f216d401       398c985c0d950       3 days ago          Exited              kube-scheduler            1                   81863d8ef01fb       kube-scheduler-minikube
01a435e92276c       c6ab243b29f82       3 days ago          Exited              kube-apiserver            1                   9f51f65570366       kube-apiserver-minikube
6a193f7efbf7c       ef43894fa110c       3 days ago          Exited              kube-controller-manager   1                   b8289c0f7a010       kube-controller-manager-minikube
8e2a1fcdf39b1       499038711c081       3 days ago          Exited              etcd                      1                   53f0ad113a955       etcd-minikube


==> coredns [1613d9f98aa7] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] 127.0.0.1:41972 - 17971 "HINFO IN 5713440188658583656.5535923875463287690. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.339586932s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error


==> coredns [70bc5c59c413] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] 127.0.0.1:39208 - 3426 "HINFO IN 4326676683665180072.88922996763041382. udp 55 false 512" NXDOMAIN qr,rd,ra 55 0.202410589s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_07_27T14_50_33_0700
                    minikube.k8s.io/version=v1.36.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 27 Jul 2025 13:50:29 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sat, 02 Aug 2025 19:54:00 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sat, 02 Aug 2025 19:49:50 +0000   Sun, 27 Jul 2025 13:50:25 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sat, 02 Aug 2025 19:49:50 +0000   Sun, 27 Jul 2025 13:50:25 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sat, 02 Aug 2025 19:49:50 +0000   Sun, 27 Jul 2025 13:50:25 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sat, 02 Aug 2025 19:49:50 +0000   Sun, 27 Jul 2025 13:50:29 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3881180Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3881180Ki
  pods:               110
System Info:
  Machine ID:                 d599b6a270554b07aeb83601808e060e
  System UUID:                d599b6a270554b07aeb83601808e060e
  Boot ID:                    fbffcb09-9c98-42ea-aed7-1d150e87d32c
  Kernel Version:             6.6.87.2-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.1.1
  Kubelet Version:            v1.33.1
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                 CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                 ------------  ----------  ---------------  -------------  ---
  default                     hello-minikube-ffcbb5874-5gx2r       0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d6h
  default                     nginx-deployment-647677fc66-rdrp4    0 (0%)        0 (0%)      0 (0%)           0 (0%)         13m
  default                     nginx-deployment-647677fc66-zb46f    0 (0%)        0 (0%)      0 (0%)           0 (0%)         13m
  kube-system                 coredns-674b8bbfcf-mblwn             100m (1%)     0 (0%)      70Mi (1%)        170Mi (4%)     6d6h
  kube-system                 etcd-minikube                        100m (1%)     0 (0%)      100Mi (2%)       0 (0%)         6d6h
  kube-system                 kube-apiserver-minikube              250m (3%)     0 (0%)      0 (0%)           0 (0%)         6d6h
  kube-system                 kube-controller-manager-minikube     200m (2%)     0 (0%)      0 (0%)           0 (0%)         6d6h
  kube-system                 kube-proxy-p84w6                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d6h
  kube-system                 kube-scheduler-minikube              100m (1%)     0 (0%)      0 (0%)           0 (0%)         6d6h
  kube-system                 storage-provisioner                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d6h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%)   0 (0%)
  memory             170Mi (4%)  170Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                   Age                    From             Message
  ----     ------                   ----                   ----             -------
  Normal   Starting                 6d6h                   kube-proxy       
  Normal   Starting                 41m                    kube-proxy       
  Normal   Starting                 3d22h                  kube-proxy       
  Normal   NodeHasSufficientPID     6d6h (x7 over 6d6h)    kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeHasSufficientMemory  6d6h (x8 over 6d6h)    kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    6d6h (x8 over 6d6h)    kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeAllocatableEnforced  6d6h                   kubelet          Updated Node Allocatable limit across pods
  Normal   Starting                 6d6h                   kubelet          Starting kubelet.
  Normal   Starting                 6d6h                   kubelet          Starting kubelet.
  Normal   NodeAllocatableEnforced  6d6h                   kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory  6d6h                   kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    6d6h                   kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     6d6h                   kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   RegisteredNode           6d6h                   node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   Starting                 3d22h                  kubelet          Starting kubelet.
  Normal   NodeAllocatableEnforced  3d22h                  kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory  3d22h (x8 over 3d22h)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    3d22h (x8 over 3d22h)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     3d22h (x7 over 3d22h)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Warning  Rebooted                 3d22h                  kubelet          Node minikube has been rebooted, boot id: 5d25a85b-5974-4264-ac26-8762151bd3af
  Normal   RegisteredNode           3d22h                  node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   Starting                 41m                    kubelet          Starting kubelet.
  Normal   NodeHasSufficientMemory  41m (x8 over 41m)      kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    41m (x8 over 41m)      kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     41m (x7 over 41m)      kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  41m                    kubelet          Updated Node Allocatable limit across pods
  Warning  Rebooted                 41m                    kubelet          Node minikube has been rebooted, boot id: fbffcb09-9c98-42ea-aed7-1d150e87d32c
  Normal   RegisteredNode           41m                    node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Aug 2 19:11] MMIO Stale Data CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/processor_mmio_stale_data.html for more details.
[  +0.007269] PCI: Fatal: No config space access function found
[  +0.028377] PCI: System does not support PCI
[  +0.143192] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +4.508590] WSL (1 - init(docker-desktop)) ERROR: ConfigApplyWindowsLibPath:2119: open /etc/ld.so.conf.d/ld.wsl.conf failed 2
[  +0.105496] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Africa/Lagos not found. Is the tzdata package installed?
[  +0.373224] pulseaudio[243]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set
[  +0.568820] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.028743] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000233] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000114] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000640] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +2.259663] WSL (214) ERROR: CheckConnection: getaddrinfo() failed: -5
[  +1.863997] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.147292] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.092734] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.048262] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.064882] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.448246] Failed to connect to bus: No such file or directory
[  +0.343414] Failed to connect to bus: No such file or directory
[  +0.290041] Failed to connect to bus: No such file or directory
[  +0.337878] Failed to connect to bus: No such file or directory
[  +0.351007] Failed to connect to bus: No such file or directory
[  +3.680444] systemd-journald[70]: File /var/log/journal/783ab1ae9c7142929bd3cf8b5fc2e064/system.journal corrupted or uncleanly shut down, renaming and replacing.
[  +4.413089] WSL (2 - init-systemd(Ubuntu)) ERROR: WaitForBootProcess:3497: /sbin/init failed to start within 10000ms
[  +5.221054] netlink: 'init': attribute type 4 has an invalid length.
[  +8.957894] ICMPv6: NA: 86:e5:35:72:e7:6a advertised our address fc00:f853:ccd:e793::2 on eth0!
[  +0.061132] ICMPv6: NA: 0a:0b:5e:04:90:18 advertised our address fc00:f853:ccd:e793::3 on eth0!
[  +0.007100] ICMPv6: NA: 4e:c6:b7:41:e3:0b advertised our address fc00:f853:ccd:e793::4 on eth0!
[  +1.024563] ICMPv6: NA: 86:e5:35:72:e7:6a advertised our address fc00:f853:ccd:e793::2 on eth0!
[  +0.061657] ICMPv6: NA: 0a:0b:5e:04:90:18 advertised our address fc00:f853:ccd:e793::3 on eth0!
[  +0.007980] ICMPv6: NA: 4e:c6:b7:41:e3:0b advertised our address fc00:f853:ccd:e793::4 on eth0!
[  +0.085733] hrtimer: interrupt took 2048748 ns
[  +0.956054] ICMPv6: NA: 86:e5:35:72:e7:6a advertised our address fc00:f853:ccd:e793::2 on eth0!
[  +0.042711] ICMPv6: NA: 0a:0b:5e:04:90:18 advertised our address fc00:f853:ccd:e793::3 on eth0!
[  +0.004000] ICMPv6: NA: 4e:c6:b7:41:e3:0b advertised our address fc00:f853:ccd:e793::4 on eth0!
[  +3.132072] ICMPv6: NA: 5e:77:02:97:69:49 advertised our address fc00:f853:ccd:e793::2 on eth0!
[  +1.092936] ICMPv6: NA: 5e:77:02:97:69:49 advertised our address fc00:f853:ccd:e793::2 on eth0!
[Aug 2 19:12] ICMPv6: NA: 5e:77:02:97:69:49 advertised our address fc00:f853:ccd:e793::2 on eth0!


==> etcd [8e2a1fcdf39b] <==
{"level":"info","ts":"2025-07-29T21:32:33.840112Z","caller":"traceutil/trace.go:171","msg":"trace[1314516825] range","detail":"{range_begin:/registry/clusterrolebindings/system:discovery; range_end:; response_count:1; response_revision:682; }","duration":"195.345307ms","start":"2025-07-29T21:32:33.644738Z","end":"2025-07-29T21:32:33.840083Z","steps":["trace[1314516825] 'agreement among raft nodes before linearized reading'  (duration: 125.899582ms)","trace[1314516825] 'range keys from bolt db'  (duration: 69.216089ms)"],"step_count":2}
{"level":"info","ts":"2025-07-29T21:32:34.743587Z","caller":"traceutil/trace.go:171","msg":"trace[1421572953] linearizableReadLoop","detail":"{readStateIndex:765; appliedIndex:763; }","duration":"180.09599ms","start":"2025-07-29T21:32:34.563452Z","end":"2025-07-29T21:32:34.743548Z","steps":["trace[1421572953] 'read index received'  (duration: 5.351615ms)","trace[1421572953] 'applied index is now lower than readState.Index'  (duration: 174.743037ms)"],"step_count":2}
{"level":"warn","ts":"2025-07-29T21:32:34.743871Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"180.37956ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterroles/system:aggregate-to-view\" limit:1 ","response":"range_response_count:1 size:1930"}
{"level":"info","ts":"2025-07-29T21:32:34.743645Z","caller":"traceutil/trace.go:171","msg":"trace[1302992042] transaction","detail":"{read_only:false; response_revision:690; number_of_response:1; }","duration":"181.22873ms","start":"2025-07-29T21:32:34.562390Z","end":"2025-07-29T21:32:34.743618Z","steps":["trace[1302992042] 'process raft request'  (duration: 85.969001ms)","trace[1302992042] 'compare'  (duration: 94.990671ms)"],"step_count":2}
{"level":"info","ts":"2025-07-29T21:32:34.743923Z","caller":"traceutil/trace.go:171","msg":"trace[617176220] range","detail":"{range_begin:/registry/clusterroles/system:aggregate-to-view; range_end:; response_count:1; response_revision:690; }","duration":"180.489901ms","start":"2025-07-29T21:32:34.563420Z","end":"2025-07-29T21:32:34.743910Z","steps":["trace[617176220] 'agreement among raft nodes before linearized reading'  (duration: 180.267882ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-29T21:32:34.744295Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"104.034882ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-07-29T21:32:34.744353Z","caller":"traceutil/trace.go:171","msg":"trace[1255200287] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:690; }","duration":"104.103536ms","start":"2025-07-29T21:32:34.640234Z","end":"2025-07-29T21:32:34.744338Z","steps":["trace[1255200287] 'agreement among raft nodes before linearized reading'  (duration: 104.007709ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-29T21:32:39.967079Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"107.17826ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/generic-garbage-collector\" limit:1 ","response":"range_response_count:1 size:216"}
{"level":"info","ts":"2025-07-29T21:32:39.967312Z","caller":"traceutil/trace.go:171","msg":"trace[1827397078] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/generic-garbage-collector; range_end:; response_count:1; response_revision:743; }","duration":"107.587151ms","start":"2025-07-29T21:32:39.859682Z","end":"2025-07-29T21:32:39.967269Z","steps":["trace[1827397078] 'range keys from in-memory index tree'  (duration: 97.540188ms)"],"step_count":1}
{"level":"info","ts":"2025-07-29T21:32:43.149350Z","caller":"traceutil/trace.go:171","msg":"trace[538641938] linearizableReadLoop","detail":"{readStateIndex:831; appliedIndex:830; }","duration":"178.439903ms","start":"2025-07-29T21:32:42.970871Z","end":"2025-07-29T21:32:43.149311Z","steps":["trace[538641938] 'read index received'  (duration: 71.295442ms)","trace[538641938] 'applied index is now lower than readState.Index'  (duration: 107.142909ms)"],"step_count":2}
{"level":"info","ts":"2025-07-29T21:32:43.149622Z","caller":"traceutil/trace.go:171","msg":"trace[935226541] transaction","detail":"{read_only:false; response_revision:753; number_of_response:1; }","duration":"183.024549ms","start":"2025-07-29T21:32:42.966477Z","end":"2025-07-29T21:32:43.149501Z","steps":["trace[935226541] 'process raft request'  (duration: 83.360487ms)","trace[935226541] 'compare'  (duration: 99.209125ms)"],"step_count":2}
{"level":"warn","ts":"2025-07-29T21:32:43.150569Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"108.262875ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/bootstrap-signer\" limit:1 ","response":"range_response_count:1 size:197"}
{"level":"info","ts":"2025-07-29T21:32:43.150640Z","caller":"traceutil/trace.go:171","msg":"trace[546033712] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/bootstrap-signer; range_end:; response_count:1; response_revision:753; }","duration":"108.403038ms","start":"2025-07-29T21:32:43.042219Z","end":"2025-07-29T21:32:43.150622Z","steps":["trace[546033712] 'agreement among raft nodes before linearized reading'  (duration: 108.245703ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-29T21:32:43.150717Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"109.301844ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/legacy-service-account-token-cleaner\" limit:1 ","response":"range_response_count:1 size:238"}
{"level":"warn","ts":"2025-07-29T21:32:43.149704Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"178.794398ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/daemon-set-controller\" limit:1 ","response":"range_response_count:1 size:207"}
{"level":"info","ts":"2025-07-29T21:32:43.150859Z","caller":"traceutil/trace.go:171","msg":"trace[1352855271] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/daemon-set-controller; range_end:; response_count:1; response_revision:753; }","duration":"180.001978ms","start":"2025-07-29T21:32:42.970829Z","end":"2025-07-29T21:32:43.150831Z","steps":["trace[1352855271] 'agreement among raft nodes before linearized reading'  (duration: 178.686611ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-29T21:32:43.154518Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"112.237317ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/replicaset-controller\" limit:1 ","response":"range_response_count:1 size:207"}
{"level":"info","ts":"2025-07-29T21:32:43.154647Z","caller":"traceutil/trace.go:171","msg":"trace[1460506342] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/replicaset-controller; range_end:; response_count:1; response_revision:753; }","duration":"112.411513ms","start":"2025-07-29T21:32:43.042204Z","end":"2025-07-29T21:32:43.154616Z","steps":["trace[1460506342] 'agreement among raft nodes before linearized reading'  (duration: 112.041295ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-29T21:32:43.154976Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"112.706634ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/service-cidrs-controller\" limit:1 ","response":"range_response_count:1 size:214"}
{"level":"info","ts":"2025-07-29T21:32:43.155052Z","caller":"traceutil/trace.go:171","msg":"trace[2068067378] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/service-cidrs-controller; range_end:; response_count:1; response_revision:753; }","duration":"112.800249ms","start":"2025-07-29T21:32:43.042230Z","end":"2025-07-29T21:32:43.155030Z","steps":["trace[2068067378] 'agreement among raft nodes before linearized reading'  (duration: 110.376502ms)"],"step_count":1}
{"level":"info","ts":"2025-07-29T21:32:43.158740Z","caller":"traceutil/trace.go:171","msg":"trace[1719712918] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/legacy-service-account-token-cleaner; range_end:; response_count:1; response_revision:753; }","duration":"110.911193ms","start":"2025-07-29T21:32:43.041299Z","end":"2025-07-29T21:32:43.152211Z","steps":["trace[1719712918] 'agreement among raft nodes before linearized reading'  (duration: 108.962762ms)"],"step_count":1}
{"level":"info","ts":"2025-07-29T21:32:43.344534Z","caller":"traceutil/trace.go:171","msg":"trace[163336373] transaction","detail":"{read_only:false; response_revision:754; number_of_response:1; }","duration":"101.223345ms","start":"2025-07-29T21:32:43.243288Z","end":"2025-07-29T21:32:43.344511Z","steps":["trace[163336373] 'process raft request'  (duration: 23.89838ms)","trace[163336373] 'compare'  (duration: 76.80548ms)"],"step_count":2}
{"level":"info","ts":"2025-07-29T21:33:12.281588Z","caller":"traceutil/trace.go:171","msg":"trace[75559700] transaction","detail":"{read_only:false; response_revision:801; number_of_response:1; }","duration":"150.867634ms","start":"2025-07-29T21:33:12.130686Z","end":"2025-07-29T21:33:12.281554Z","steps":["trace[75559700] 'process raft request'  (duration: 108.836129ms)","trace[75559700] 'compare'  (duration: 38.871951ms)"],"step_count":2}
{"level":"info","ts":"2025-07-29T21:33:12.281745Z","caller":"traceutil/trace.go:171","msg":"trace[1366166121] transaction","detail":"{read_only:false; response_revision:802; number_of_response:1; }","duration":"150.899375ms","start":"2025-07-29T21:33:12.130825Z","end":"2025-07-29T21:33:12.281725Z","steps":["trace[1366166121] 'process raft request'  (duration: 150.520444ms)"],"step_count":1}
{"level":"info","ts":"2025-07-29T21:33:12.472957Z","caller":"traceutil/trace.go:171","msg":"trace[113636394] transaction","detail":"{read_only:false; response_revision:809; number_of_response:1; }","duration":"103.953756ms","start":"2025-07-29T21:33:12.368963Z","end":"2025-07-29T21:33:12.472916Z","steps":["trace[113636394] 'process raft request'  (duration: 93.295806ms)"],"step_count":1}
{"level":"info","ts":"2025-07-29T21:34:14.185163Z","caller":"traceutil/trace.go:171","msg":"trace[481860483] transaction","detail":"{read_only:false; response_revision:867; number_of_response:1; }","duration":"101.733417ms","start":"2025-07-29T21:34:14.083395Z","end":"2025-07-29T21:34:14.185129Z","steps":["trace[481860483] 'process raft request'  (duration: 101.511738ms)"],"step_count":1}
{"level":"warn","ts":"2025-07-29T21:40:18.432171Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"170.73679ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128038931653894441 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:1171 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:599 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >>","response":"size:16"}
{"level":"info","ts":"2025-07-29T21:40:18.433609Z","caller":"traceutil/trace.go:171","msg":"trace[100184073] transaction","detail":"{read_only:false; response_revision:1179; number_of_response:1; }","duration":"207.820331ms","start":"2025-07-29T21:40:18.225600Z","end":"2025-07-29T21:40:18.433420Z","steps":["trace[100184073] 'process raft request'  (duration: 15.39606ms)","trace[100184073] 'store kv pair into bolt db' {req_type:put; key:/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii; req_size:669; } (duration: 154.774039ms)"],"step_count":2}
{"level":"warn","ts":"2025-07-29T21:40:18.438290Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"105.594547ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-07-29T21:40:18.439056Z","caller":"traceutil/trace.go:171","msg":"trace[1432626708] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1179; }","duration":"105.821161ms","start":"2025-07-29T21:40:18.332611Z","end":"2025-07-29T21:40:18.438432Z","steps":["trace[1432626708] 'agreement among raft nodes before linearized reading'  (duration: 105.5612ms)"],"step_count":1}
{"level":"info","ts":"2025-07-29T21:40:18.447283Z","caller":"traceutil/trace.go:171","msg":"trace[919384433] linearizableReadLoop","detail":"{readStateIndex:1353; appliedIndex:1351; }","duration":"105.236093ms","start":"2025-07-29T21:40:18.332727Z","end":"2025-07-29T21:40:18.437963Z","steps":["trace[919384433] 'read index received'  (duration: 77.745628ms)","trace[919384433] 'applied index is now lower than readState.Index'  (duration: 27.48871ms)"],"step_count":2}
{"level":"info","ts":"2025-07-29T21:42:07.429103Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1028}
{"level":"info","ts":"2025-07-29T21:42:07.632144Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":1028,"took":"201.081633ms","hash":2919525115,"current-db-size-bytes":2961408,"current-db-size":"3.0 MB","current-db-size-in-use-bytes":1187840,"current-db-size-in-use":"1.2 MB"}
{"level":"info","ts":"2025-07-29T21:42:07.632320Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2919525115,"revision":1028,"compact-revision":-1}
{"level":"info","ts":"2025-07-29T21:46:57.266139Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1267}
{"level":"info","ts":"2025-07-29T21:46:57.276145Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":1267,"took":"8.773553ms","hash":1636743963,"current-db-size-bytes":2961408,"current-db-size":"3.0 MB","current-db-size-in-use-bytes":1609728,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2025-07-29T21:46:57.276365Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1636743963,"revision":1267,"compact-revision":1028}
{"level":"info","ts":"2025-07-29T21:51:46.940092Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1506}
{"level":"info","ts":"2025-07-29T21:51:46.953658Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":1506,"took":"11.741376ms","hash":2284351901,"current-db-size-bytes":2961408,"current-db-size":"3.0 MB","current-db-size-in-use-bytes":1671168,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-07-29T21:51:46.953796Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2284351901,"revision":1506,"compact-revision":1267}
{"level":"info","ts":"2025-07-29T21:55:45.363405Z","caller":"traceutil/trace.go:171","msg":"trace[1486772710] transaction","detail":"{read_only:false; response_revision:1949; number_of_response:1; }","duration":"114.387303ms","start":"2025-07-29T21:55:45.244816Z","end":"2025-07-29T21:55:45.359204Z","steps":["trace[1486772710] 'process raft request'  (duration: 111.742444ms)"],"step_count":1}
{"level":"info","ts":"2025-07-29T21:56:36.545022Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1751}
{"level":"info","ts":"2025-07-29T21:56:36.557312Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":1751,"took":"11.435358ms","hash":2928351659,"current-db-size-bytes":2961408,"current-db-size":"3.0 MB","current-db-size-in-use-bytes":1728512,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-07-29T21:56:36.557486Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2928351659,"revision":1751,"compact-revision":1506}
{"level":"info","ts":"2025-07-29T22:01:25.460501Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1990}
{"level":"info","ts":"2025-07-29T22:01:25.469173Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":1990,"took":"8.296755ms","hash":2889785191,"current-db-size-bytes":2961408,"current-db-size":"3.0 MB","current-db-size-in-use-bytes":1703936,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-07-29T22:01:25.469248Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2889785191,"revision":1990,"compact-revision":1751}
{"level":"info","ts":"2025-07-29T22:06:15.301881Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2236}
{"level":"info","ts":"2025-07-29T22:06:15.347112Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":2236,"took":"44.616327ms","hash":4189762542,"current-db-size-bytes":2961408,"current-db-size":"3.0 MB","current-db-size-in-use-bytes":1773568,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-07-29T22:06:15.347278Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":4189762542,"revision":2236,"compact-revision":1990}
{"level":"info","ts":"2025-07-29T22:06:38.649214Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-07-29T22:06:38.737560Z","caller":"embed/etcd.go:408","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"info","ts":"2025-07-29T22:06:45.755221Z","caller":"etcdserver/server.go:1546","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"warn","ts":"2025-07-29T22:06:45.774862Z","caller":"embed/serve.go:235","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-07-29T22:06:45.776524Z","caller":"embed/serve.go:237","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-07-29T22:06:45.772978Z","caller":"embed/serve.go:235","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-07-29T22:06:45.841114Z","caller":"embed/serve.go:237","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2025-07-29T22:06:45.854118Z","caller":"embed/etcd.go:613","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-07-29T22:06:45.857222Z","caller":"embed/etcd.go:618","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-07-29T22:06:45.857424Z","caller":"embed/etcd.go:410","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> etcd [95223a401b60] <==
{"level":"info","ts":"2025-08-02T19:43:18.777941Z","caller":"traceutil/trace.go:171","msg":"trace[510692909] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:4022; }","duration":"178.533047ms","start":"2025-08-02T19:43:18.599376Z","end":"2025-08-02T19:43:18.777909Z","steps":["trace[510692909] 'agreement among raft nodes before linearized reading'  (duration: 87.162704ms)"],"step_count":1}
{"level":"info","ts":"2025-08-02T19:43:18.594020Z","caller":"traceutil/trace.go:171","msg":"trace[540806932] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:4022; }","duration":"807.924952ms","start":"2025-08-02T19:43:17.786037Z","end":"2025-08-02T19:43:18.593962Z","steps":["trace[540806932] 'agreement among raft nodes before linearized reading'  (duration: 693.100969ms)","trace[540806932] 'get authentication metadata'  (duration: 98.648782ms)"],"step_count":2}
{"level":"warn","ts":"2025-08-02T19:43:18.987529Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"295.364782ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-08-02T19:43:18.988166Z","caller":"traceutil/trace.go:171","msg":"trace[1859248215] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:4022; }","duration":"302.737568ms","start":"2025-08-02T19:43:18.685143Z","end":"2025-08-02T19:43:18.987881Z","steps":["trace[1859248215] 'get authentication metadata'  (duration: 176.227725ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-02T19:43:18.988518Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-08-02T19:43:18.685098Z","time spent":"303.36989ms","remote":"127.0.0.1:44802","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"info","ts":"2025-08-02T19:43:19.790506Z","caller":"traceutil/trace.go:171","msg":"trace[1310031466] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:4022; }","duration":"105.616942ms","start":"2025-08-02T19:43:19.684828Z","end":"2025-08-02T19:43:19.790445Z","steps":["trace[1310031466] 'agreement among raft nodes before linearized reading'  (duration: 105.572298ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-02T19:43:26.677405Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"188.092806ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128039017976498390 > lease_revoke:<id:70cc986c3301088f>","response":"size:29"}
{"level":"info","ts":"2025-08-02T19:43:26.678968Z","caller":"traceutil/trace.go:171","msg":"trace[1358608081] transaction","detail":"{read_only:false; response_revision:4028; number_of_response:1; }","duration":"275.716069ms","start":"2025-08-02T19:43:26.403209Z","end":"2025-08-02T19:43:26.678925Z","steps":["trace[1358608081] 'process raft request'  (duration: 274.528384ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-02T19:43:26.682787Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"287.208711ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2025-08-02T19:43:26.682886Z","caller":"traceutil/trace.go:171","msg":"trace[1083371042] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:4028; }","duration":"287.385564ms","start":"2025-08-02T19:43:26.395468Z","end":"2025-08-02T19:43:26.682854Z","steps":["trace[1083371042] 'agreement among raft nodes before linearized reading'  (duration: 287.061876ms)"],"step_count":1}
{"level":"info","ts":"2025-08-02T19:43:26.677937Z","caller":"traceutil/trace.go:171","msg":"trace[960002154] linearizableReadLoop","detail":"{readStateIndex:4914; appliedIndex:4913; }","duration":"282.296225ms","start":"2025-08-02T19:43:26.395524Z","end":"2025-08-02T19:43:26.677820Z","steps":["trace[960002154] 'read index received'  (duration: 94.330848ms)","trace[960002154] 'applied index is now lower than readState.Index'  (duration: 187.96098ms)"],"step_count":2}
{"level":"warn","ts":"2025-08-02T19:43:26.896217Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"108.680951ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-08-02T19:43:26.896351Z","caller":"traceutil/trace.go:171","msg":"trace[1461696778] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:4028; }","duration":"108.897284ms","start":"2025-08-02T19:43:26.787424Z","end":"2025-08-02T19:43:26.896322Z","steps":["trace[1461696778] 'range keys from in-memory index tree'  (duration: 108.44024ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-02T19:43:26.896727Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"214.274151ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-08-02T19:43:26.896793Z","caller":"traceutil/trace.go:171","msg":"trace[1950498005] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:4028; }","duration":"214.385998ms","start":"2025-08-02T19:43:26.682384Z","end":"2025-08-02T19:43:26.896771Z","steps":["trace[1950498005] 'agreement among raft nodes before linearized reading'  (duration: 99.261975ms)","trace[1950498005] 'range keys from in-memory index tree'  (duration: 114.99688ms)"],"step_count":2}
{"level":"warn","ts":"2025-08-02T19:43:27.678005Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"186.818034ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/deployments/\" range_end:\"/registry/deployments0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"info","ts":"2025-08-02T19:43:27.678372Z","caller":"traceutil/trace.go:171","msg":"trace[516904628] range","detail":"{range_begin:/registry/deployments/; range_end:/registry/deployments0; response_count:0; response_revision:4029; }","duration":"187.231105ms","start":"2025-08-02T19:43:27.491096Z","end":"2025-08-02T19:43:27.678327Z","steps":["trace[516904628] 'agreement among raft nodes before linearized reading'  (duration: 186.783429ms)"],"step_count":1}
{"level":"info","ts":"2025-08-02T19:43:27.598742Z","caller":"traceutil/trace.go:171","msg":"trace[272925905] transaction","detail":"{read_only:false; response_revision:4029; number_of_response:1; }","duration":"108.124677ms","start":"2025-08-02T19:43:27.490564Z","end":"2025-08-02T19:43:27.598689Z","steps":["trace[272925905] 'process raft request'  (duration: 106.357299ms)"],"step_count":1}
{"level":"info","ts":"2025-08-02T19:43:27.685358Z","caller":"traceutil/trace.go:171","msg":"trace[1838488094] linearizableReadLoop","detail":"{readStateIndex:4916; appliedIndex:4915; }","duration":"194.157892ms","start":"2025-08-02T19:43:27.491150Z","end":"2025-08-02T19:43:27.685308Z","steps":["trace[1838488094] 'read index received'  (duration: 93.695136ms)","trace[1838488094] 'applied index is now lower than readState.Index'  (duration: 13.673091ms)"],"step_count":2}
{"level":"info","ts":"2025-08-02T19:43:40.038454Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3781}
{"level":"info","ts":"2025-08-02T19:43:40.147508Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":3781,"took":"103.303906ms","hash":3418892117,"current-db-size-bytes":2961408,"current-db-size":"3.0 MB","current-db-size-in-use-bytes":1720320,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-08-02T19:43:40.147735Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3418892117,"revision":3781,"compact-revision":3539}
{"level":"warn","ts":"2025-08-02T19:45:04.006845Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"117.170021ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-08-02T19:45:04.009043Z","caller":"traceutil/trace.go:171","msg":"trace[1424322676] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:4106; }","duration":"120.993383ms","start":"2025-08-02T19:45:03.887850Z","end":"2025-08-02T19:45:04.008843Z","steps":["trace[1424322676] 'agreement among raft nodes before linearized reading'  (duration: 96.247944ms)","trace[1424322676] 'range keys from in-memory index tree'  (duration: 20.704395ms)"],"step_count":2}
{"level":"info","ts":"2025-08-02T19:45:04.312290Z","caller":"traceutil/trace.go:171","msg":"trace[879528818] transaction","detail":"{read_only:false; response_revision:4110; number_of_response:1; }","duration":"100.922157ms","start":"2025-08-02T19:45:04.211313Z","end":"2025-08-02T19:45:04.312235Z","steps":["trace[879528818] 'process raft request'  (duration: 92.608218ms)"],"step_count":1}
{"level":"info","ts":"2025-08-02T19:46:31.078830Z","caller":"traceutil/trace.go:171","msg":"trace[1297968229] transaction","detail":"{read_only:false; response_revision:4176; number_of_response:1; }","duration":"145.108368ms","start":"2025-08-02T19:46:30.933672Z","end":"2025-08-02T19:46:31.078780Z","steps":["trace[1297968229] 'process raft request'  (duration: 144.806672ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-02T19:48:49.494280Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"111.435633ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-08-02T19:48:49.494551Z","caller":"traceutil/trace.go:171","msg":"trace[1205136601] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:4280; }","duration":"147.840552ms","start":"2025-08-02T19:48:49.346665Z","end":"2025-08-02T19:48:49.494506Z","steps":["trace[1205136601] 'range keys from in-memory index tree'  (duration: 111.151061ms)"],"step_count":1}
{"level":"info","ts":"2025-08-02T19:48:54.096304Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":4038}
{"level":"info","ts":"2025-08-02T19:48:54.218722Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":4038,"took":"112.984255ms","hash":2082237624,"current-db-size-bytes":2961408,"current-db-size":"3.0 MB","current-db-size-in-use-bytes":1839104,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-08-02T19:48:54.220313Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2082237624,"revision":4038,"compact-revision":3781}
{"level":"info","ts":"2025-08-02T19:51:02.240645Z","caller":"traceutil/trace.go:171","msg":"trace[1984650928] transaction","detail":"{read_only:false; response_revision:4381; number_of_response:1; }","duration":"228.516321ms","start":"2025-08-02T19:51:02.012082Z","end":"2025-08-02T19:51:02.240598Z","steps":["trace[1984650928] 'process raft request'  (duration: 166.622355ms)","trace[1984650928] 'compare'  (duration: 61.502323ms)"],"step_count":2}
{"level":"info","ts":"2025-08-02T19:51:02.814320Z","caller":"traceutil/trace.go:171","msg":"trace[499326108] transaction","detail":"{read_only:false; response_revision:4382; number_of_response:1; }","duration":"191.231475ms","start":"2025-08-02T19:51:02.623035Z","end":"2025-08-02T19:51:02.814266Z","steps":["trace[499326108] 'process raft request'  (duration: 190.676584ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-02T19:51:08.782192Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"124.512857ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128039017976500545 > lease_revoke:<id:70cc986c330110f8>","response":"size:29"}
{"level":"info","ts":"2025-08-02T19:51:08.981478Z","caller":"traceutil/trace.go:171","msg":"trace[650163261] transaction","detail":"{read_only:false; response_revision:4385; number_of_response:1; }","duration":"132.880642ms","start":"2025-08-02T19:51:08.848474Z","end":"2025-08-02T19:51:08.981355Z","steps":["trace[650163261] 'process raft request'  (duration: 132.378882ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-02T19:51:41.853980Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"128.654139ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-08-02T19:51:41.854315Z","caller":"traceutil/trace.go:171","msg":"trace[342540229] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:4410; }","duration":"130.985099ms","start":"2025-08-02T19:51:41.723167Z","end":"2025-08-02T19:51:41.854152Z","steps":["trace[342540229] 'agreement among raft nodes before linearized reading'  (duration: 92.828724ms)","trace[342540229] 'range keys from in-memory index tree'  (duration: 35.854507ms)"],"step_count":2}
{"level":"info","ts":"2025-08-02T19:51:41.920056Z","caller":"traceutil/trace.go:171","msg":"trace[66892920] transaction","detail":"{read_only:false; response_revision:4410; number_of_response:1; }","duration":"177.870985ms","start":"2025-08-02T19:51:41.640974Z","end":"2025-08-02T19:51:41.818845Z","steps":["trace[66892920] 'process raft request'  (duration: 117.732833ms)","trace[66892920] 'store kv pair into bolt db' {req_type:put; key:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; req_size:1090; } (duration: 57.083525ms)"],"step_count":2}
{"level":"warn","ts":"2025-08-02T19:52:42.885255Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"193.555672ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128039017976500981 > lease_revoke:<id:70cc986c330112ac>","response":"size:29"}
{"level":"warn","ts":"2025-08-02T19:52:44.063503Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"525.755927ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-08-02T19:52:44.063755Z","caller":"traceutil/trace.go:171","msg":"trace[1644561986] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:4454; }","duration":"526.21833ms","start":"2025-08-02T19:52:41.867469Z","end":"2025-08-02T19:52:44.063674Z","steps":["trace[1644561986] 'agreement among raft nodes before linearized reading'  (duration: 525.763268ms)"],"step_count":1}
{"level":"info","ts":"2025-08-02T19:52:44.064164Z","caller":"traceutil/trace.go:171","msg":"trace[194080972] linearizableReadLoop","detail":"{readStateIndex:5449; appliedIndex:5448; }","duration":"525.446165ms","start":"2025-08-02T19:52:41.867537Z","end":"2025-08-02T19:52:44.062971Z","steps":["trace[194080972] 'read index received'  (duration: 208.392331ms)","trace[194080972] 'applied index is now lower than readState.Index'  (duration: 317.050401ms)"],"step_count":2}
{"level":"warn","ts":"2025-08-02T19:52:44.133261Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"289.510812ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2025-08-02T19:52:44.133374Z","caller":"traceutil/trace.go:171","msg":"trace[405268750] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:4454; }","duration":"289.758508ms","start":"2025-08-02T19:52:42.789201Z","end":"2025-08-02T19:52:44.133333Z","steps":["trace[405268750] 'agreement among raft nodes before linearized reading'  (duration: 289.513387ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-02T19:52:45.058964Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-08-02T19:52:42.789158Z","time spent":"1.213874858s","remote":"127.0.0.1:44938","response type":"/etcdserverpb.KV/Range","request count":0,"request size":69,"response count":1,"response size":1133,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 "}
{"level":"info","ts":"2025-08-02T19:53:40.114115Z","caller":"traceutil/trace.go:171","msg":"trace[1643919613] transaction","detail":"{read_only:false; response_revision:4497; number_of_response:1; }","duration":"188.015641ms","start":"2025-08-02T19:53:39.926032Z","end":"2025-08-02T19:53:40.114048Z","steps":["trace[1643919613] 'process raft request'  (duration: 140.384694ms)","trace[1643919613] 'compare'  (duration: 46.892339ms)"],"step_count":2}
{"level":"info","ts":"2025-08-02T19:53:42.609982Z","caller":"traceutil/trace.go:171","msg":"trace[1519696065] transaction","detail":"{read_only:false; response_revision:4499; number_of_response:1; }","duration":"210.582224ms","start":"2025-08-02T19:53:42.399176Z","end":"2025-08-02T19:53:42.609758Z","steps":["trace[1519696065] 'process raft request'  (duration: 131.63751ms)","trace[1519696065] 'store kv pair into bolt db' {req_type:put; key:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; req_size:1090; } (duration: 78.703552ms)"],"step_count":2}
{"level":"warn","ts":"2025-08-02T19:53:44.217053Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"119.129656ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-08-02T19:53:44.216448Z","caller":"traceutil/trace.go:171","msg":"trace[1658678851] transaction","detail":"{read_only:false; response_revision:4500; number_of_response:1; }","duration":"136.377354ms","start":"2025-08-02T19:53:44.079950Z","end":"2025-08-02T19:53:44.216327Z","steps":["trace[1658678851] 'process raft request'  (duration: 132.17099ms)"],"step_count":1}
{"level":"info","ts":"2025-08-02T19:53:44.217214Z","caller":"traceutil/trace.go:171","msg":"trace[604266125] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:4500; }","duration":"119.315542ms","start":"2025-08-02T19:53:44.097868Z","end":"2025-08-02T19:53:44.217183Z","steps":["trace[604266125] 'agreement among raft nodes before linearized reading'  (duration: 119.078232ms)"],"step_count":1}
{"level":"info","ts":"2025-08-02T19:53:44.212464Z","caller":"traceutil/trace.go:171","msg":"trace[1857980117] linearizableReadLoop","detail":"{readStateIndex:5506; appliedIndex:5505; }","duration":"114.530378ms","start":"2025-08-02T19:53:44.097881Z","end":"2025-08-02T19:53:44.212411Z","steps":["trace[1857980117] 'read index received'  (duration: 114.145747ms)","trace[1857980117] 'applied index is now lower than readState.Index'  (duration: 383.583Âµs)"],"step_count":2}
{"level":"warn","ts":"2025-08-02T19:53:44.228234Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"114.227359ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/podtemplates/\" range_end:\"/registry/podtemplates0\" count_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-08-02T19:53:44.228422Z","caller":"traceutil/trace.go:171","msg":"trace[982783963] range","detail":"{range_begin:/registry/podtemplates/; range_end:/registry/podtemplates0; response_count:0; response_revision:4500; }","duration":"114.485239ms","start":"2025-08-02T19:53:44.113904Z","end":"2025-08-02T19:53:44.228389Z","steps":["trace[982783963] 'agreement among raft nodes before linearized reading'  (duration: 114.197171ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-02T19:53:45.167861Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"131.756268ms","expected-duration":"100ms","prefix":"","request":"header:<ID:8128039017976501284 > lease_revoke:<id:70cc986c330113d7>","response":"size:29"}
{"level":"warn","ts":"2025-08-02T19:53:45.223239Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"335.070797ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-08-02T19:53:45.223614Z","caller":"traceutil/trace.go:171","msg":"trace[1376991409] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:4500; }","duration":"335.300586ms","start":"2025-08-02T19:53:44.888041Z","end":"2025-08-02T19:53:45.223341Z","steps":["trace[1376991409] 'agreement among raft nodes before linearized reading'  (duration: 335.079845ms)"],"step_count":1}
{"level":"info","ts":"2025-08-02T19:53:45.168057Z","caller":"traceutil/trace.go:171","msg":"trace[198990299] linearizableReadLoop","detail":"{readStateIndex:5507; appliedIndex:5506; }","duration":"279.840416ms","start":"2025-08-02T19:53:44.888185Z","end":"2025-08-02T19:53:45.168026Z","steps":["trace[198990299] 'read index received'  (duration: 148.569531ms)","trace[198990299] 'applied index is now lower than readState.Index'  (duration: 131.267838ms)"],"step_count":2}
{"level":"warn","ts":"2025-08-02T19:53:45.302959Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"223.064372ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" limit:1 ","response":"range_response_count:1 size:1109"}
{"level":"info","ts":"2025-08-02T19:53:45.303306Z","caller":"traceutil/trace.go:171","msg":"trace[1881405242] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:4500; }","duration":"223.421481ms","start":"2025-08-02T19:53:45.079641Z","end":"2025-08-02T19:53:45.303062Z","steps":["trace[1881405242] 'agreement among raft nodes before linearized reading'  (duration: 223.011519ms)"],"step_count":1}
{"level":"warn","ts":"2025-08-02T19:53:45.223713Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-08-02T19:53:44.888007Z","time spent":"335.682168ms","remote":"127.0.0.1:44802","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}


==> kernel <==
 19:54:05 up 42 min,  0 users,  load average: 8.22, 3.15, 2.19
Linux minikube 6.6.87.2-microsoft-standard-WSL2 #1 SMP PREEMPT_DYNAMIC Thu Jun  5 18:30:46 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [01a435e92276] <==
W0729 22:06:44.317682       1 logging.go:55] [core] [Channel #82 SubChannel #83]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:44.317849       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:44.326600       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:44.355662       1 logging.go:55] [core] [Channel #19 SubChannel #20]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:44.380737       1 logging.go:55] [core] [Channel #22 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:44.387948       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:44.410666       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:44.431189       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:44.431860       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:44.451467       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:44.476707       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:44.483707       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:44.497708       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:44.505499       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:44.573616       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:44.608555       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:44.643130       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:44.655179       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:44.655964       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:44.674807       1 logging.go:55] [core] [Channel #2 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:44.681867       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:44.733688       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:44.810376       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:47.210346       1 logging.go:55] [core] [Channel #7 SubChannel #8]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:47.282233       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:47.323530       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:47.344869       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:47.484401       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:47.523428       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:47.534870       1 logging.go:55] [core] [Channel #1 SubChannel #4]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:47.562759       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:47.584146       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:47.603509       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:47.681841       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:47.696552       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:47.704989       1 logging.go:55] [core] [Channel #148 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:47.749562       1 logging.go:55] [core] [Channel #157 SubChannel #158]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:47.786686       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:47.812982       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:47.838322       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:47.867208       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:47.873493       1 logging.go:55] [core] [Channel #22 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:47.884258       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:47.906856       1 logging.go:55] [core] [Channel #25 SubChannel #26]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:47.958350       1 logging.go:55] [core] [Channel #184 SubChannel #185]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:47.979981       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:48.007561       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:48.025346       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:48.102582       1 logging.go:55] [core] [Channel #19 SubChannel #20]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:48.103352       1 logging.go:55] [core] [Channel #181 SubChannel #182]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:48.106347       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:48.143396       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:48.157739       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:48.173201       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:48.189523       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:48.210155       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:48.229058       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:48.281893       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:48.291906       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0729 22:06:48.300888       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-apiserver [ac4627797f8e] <==
I0802 19:12:28.142807       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0802 19:12:28.141356       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0802 19:12:28.144020       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0802 19:12:28.144099       1 crd_finalizer.go:269] Starting CRDFinalizer
I0802 19:12:28.146693       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0802 19:12:28.146737       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0802 19:12:28.146795       1 controller.go:78] Starting OpenAPI AggregationController
I0802 19:12:28.156444       1 cluster_authentication_trust_controller.go:459] Starting cluster_authentication_trust_controller controller
I0802 19:12:28.156486       1 shared_informer.go:350] "Waiting for caches to sync" controller="cluster_authentication_trust_controller"
I0802 19:12:28.157407       1 default_servicecidr_controller.go:110] Starting kubernetes-service-cidr-controller
I0802 19:12:28.157472       1 shared_informer.go:350] "Waiting for caches to sync" controller="kubernetes-service-cidr-controller"
I0802 19:12:28.160353       1 controller.go:142] Starting OpenAPI controller
I0802 19:12:28.160523       1 controller.go:90] Starting OpenAPI V3 controller
I0802 19:12:28.160589       1 naming_controller.go:299] Starting NamingConditionController
I0802 19:12:28.160620       1 establishing_controller.go:81] Starting EstablishingController
I0802 19:12:28.160690       1 repairip.go:200] Starting ipallocator-repair-controller
I0802 19:12:28.160704       1 shared_informer.go:350] "Waiting for caches to sync" controller="ipallocator-repair-controller"
I0802 19:12:28.161224       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0802 19:12:28.161417       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0802 19:12:28.173698       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0802 19:12:28.179520       1 controller.go:119] Starting legacy_token_tracking_controller
I0802 19:12:28.179654       1 shared_informer.go:350] "Waiting for caches to sync" controller="configmaps"
I0802 19:12:28.449760       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I0802 19:12:28.455681       1 shared_informer.go:357] "Caches are synced" controller="configmaps"
I0802 19:12:28.459250       1 shared_informer.go:357] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I0802 19:12:28.459294       1 policy_source.go:240] refreshing policies
I0802 19:12:28.459676       1 cache.go:39] Caches are synced for LocalAvailability controller
I0802 19:12:28.460863       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0802 19:12:28.462029       1 shared_informer.go:357] "Caches are synced" controller="cluster_authentication_trust_controller"
I0802 19:12:28.462045       1 shared_informer.go:357] "Caches are synced" controller="ipallocator-repair-controller"
I0802 19:12:28.462346       1 shared_informer.go:357] "Caches are synced" controller="kubernetes-service-cidr-controller"
I0802 19:12:28.463815       1 default_servicecidr_controller.go:136] Shutting down kubernetes-service-cidr-controller
I0802 19:12:28.475142       1 shared_informer.go:357] "Caches are synced" controller="node_authorizer"
I0802 19:12:28.484039       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0802 19:12:28.542261       1 shared_informer.go:357] "Caches are synced" controller="crd-autoregister"
I0802 19:12:28.542296       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0802 19:12:28.542309       1 aggregator.go:171] initial CRD sync complete...
I0802 19:12:28.542313       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0802 19:12:28.542321       1 autoregister_controller.go:144] Starting autoregister controller
I0802 19:12:28.542328       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0802 19:12:28.542336       1 cache.go:39] Caches are synced for autoregister controller
I0802 19:12:28.547585       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0802 19:12:28.550380       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
E0802 19:12:28.572688       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 3a5ea174-2cc7-4c71-a7eb-99be85b19739, UID in object meta: "
I0802 19:12:29.246596       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I0802 19:12:29.448789       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
W0802 19:12:35.452811       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0802 19:12:35.456189       1 controller.go:667] quota admission added evaluator for: endpoints
I0802 19:12:40.039590       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0802 19:12:40.139852       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0802 19:12:40.266737       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0802 19:12:40.341707       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I0802 19:12:40.555443       1 controller.go:667] quota admission added evaluator for: deployments.apps
I0802 19:15:54.145614       1 controller.go:667] quota admission added evaluator for: namespaces
I0802 19:22:48.204563       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0802 19:33:14.618065       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0802 19:43:44.931119       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0802 19:45:03.710652       1 alloc.go:328] "allocated clusterIPs" service="dev/nginx" clusterIPs={"IPv4":"10.105.181.109"}
I0802 19:45:04.001044       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0802 19:45:04.207919       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12


==> kube-controller-manager [5cc9688ddb82] <==
I0802 19:12:38.757273       1 controllermanager.go:741] "Warning: controller is disabled" controller="selinux-warning-controller"
I0802 19:12:38.757795       1 pv_protection_controller.go:81] "Starting PV protection controller" logger="persistentvolume-protection-controller"
I0802 19:12:38.757837       1 shared_informer.go:350] "Waiting for caches to sync" controller="PV protection"
I0802 19:12:38.771845       1 controllermanager.go:778] "Started controller" controller="endpointslice-controller"
I0802 19:12:38.773038       1 endpointslice_controller.go:281] "Starting endpoint slice controller" logger="endpointslice-controller"
I0802 19:12:38.773262       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint_slice"
I0802 19:12:38.878747       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0802 19:12:38.959677       1 shared_informer.go:357] "Caches are synced" controller="TTL"
I0802 19:12:39.039639       1 shared_informer.go:350] "Waiting for caches to sync" controller="resource quota"
I0802 19:12:39.040899       1 shared_informer.go:357] "Caches are synced" controller="node"
I0802 19:12:39.041408       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0802 19:12:39.041560       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0802 19:12:39.041653       1 shared_informer.go:350] "Waiting for caches to sync" controller="cidrallocator"
I0802 19:12:39.041678       1 shared_informer.go:357] "Caches are synced" controller="cidrallocator"
I0802 19:12:39.159701       1 shared_informer.go:357] "Caches are synced" controller="TTL after finished"
I0802 19:12:39.234526       1 shared_informer.go:357] "Caches are synced" controller="validatingadmissionpolicy-status"
I0802 19:12:39.249517       1 shared_informer.go:357] "Caches are synced" controller="expand"
I0802 19:12:39.335034       1 shared_informer.go:357] "Caches are synced" controller="service account"
I0802 19:12:39.337765       1 shared_informer.go:357] "Caches are synced" controller="namespace"
I0802 19:12:39.345611       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0802 19:12:39.347297       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0802 19:12:39.350309       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0802 19:12:39.354792       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0802 19:12:39.357905       1 shared_informer.go:357] "Caches are synced" controller="PV protection"
I0802 19:12:39.358143       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrapproving"
I0802 19:12:39.358430       1 shared_informer.go:357] "Caches are synced" controller="bootstrap_signer"
I0802 19:12:39.362507       1 shared_informer.go:357] "Caches are synced" controller="crt configmap"
I0802 19:12:39.437339       1 shared_informer.go:357] "Caches are synced" controller="cronjob"
I0802 19:12:39.444197       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0802 19:12:39.452390       1 shared_informer.go:357] "Caches are synced" controller="service-cidr-controller"
I0802 19:12:39.538929       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0802 19:12:39.539010       1 shared_informer.go:357] "Caches are synced" controller="job"
I0802 19:12:39.540163       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0802 19:12:39.540551       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0802 19:12:39.543235       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0802 19:12:39.544505       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0802 19:12:39.545833       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0802 19:12:39.545939       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0802 19:12:39.548049       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0802 19:12:39.551228       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0802 19:12:39.557343       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0802 19:12:39.557743       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0802 19:12:39.544641       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0802 19:12:39.559603       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0802 19:12:39.639457       1 shared_informer.go:357] "Caches are synced" controller="ReplicaSet"
I0802 19:12:39.559909       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0802 19:12:39.644522       1 shared_informer.go:357] "Caches are synced" controller="stateful set"
I0802 19:12:39.649595       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0802 19:12:39.649704       1 shared_informer.go:357] "Caches are synced" controller="disruption"
I0802 19:12:39.651737       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0802 19:12:39.653375       1 shared_informer.go:357] "Caches are synced" controller="deployment"
I0802 19:12:39.659245       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0802 19:12:39.660065       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0802 19:12:39.838387       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0802 19:12:39.934639       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0802 19:12:40.247989       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/hello-minikube" err="EndpointSlice informer cache is out of date"
I0802 19:12:40.334367       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0802 19:12:40.334424       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0802 19:12:40.334444       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0802 19:12:40.335351       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"


==> kube-controller-manager [6a193f7efbf7] <==
I0729 21:32:42.476005       1 shared_informer.go:350] "Waiting for caches to sync" controller="node"
I0729 21:32:42.541153       1 controllermanager.go:778] "Started controller" controller="root-ca-certificate-publisher-controller"
I0729 21:32:42.541775       1 publisher.go:107] "Starting root CA cert publisher controller" logger="root-ca-certificate-publisher-controller"
I0729 21:32:42.541888       1 shared_informer.go:350] "Waiting for caches to sync" controller="crt configmap"
I0729 21:32:42.555134       1 controllermanager.go:778] "Started controller" controller="service-cidr-controller"
I0729 21:32:42.557604       1 servicecidrs_controller.go:136] "Starting" logger="service-cidr-controller" controller="service-cidr-controller"
I0729 21:32:42.557660       1 shared_informer.go:350] "Waiting for caches to sync" controller="service-cidr-controller"
I0729 21:32:42.583761       1 shared_informer.go:350] "Waiting for caches to sync" controller="resource quota"
I0729 21:32:42.763350       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0729 21:32:42.764449       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0729 21:32:42.765126       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0729 21:32:42.765585       1 shared_informer.go:357] "Caches are synced" controller="service-cidr-controller"
I0729 21:32:42.766051       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrapproving"
I0729 21:32:42.767013       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0729 21:32:42.771955       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0729 21:32:42.845123       1 shared_informer.go:357] "Caches are synced" controller="namespace"
I0729 21:32:42.845253       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0729 21:32:42.845484       1 shared_informer.go:357] "Caches are synced" controller="TTL after finished"
I0729 21:32:42.846157       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0729 21:32:42.846287       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0729 21:32:42.857086       1 shared_informer.go:357] "Caches are synced" controller="service account"
I0729 21:32:42.857188       1 shared_informer.go:357] "Caches are synced" controller="node"
I0729 21:32:42.857271       1 shared_informer.go:357] "Caches are synced" controller="bootstrap_signer"
I0729 21:32:42.859071       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0729 21:32:42.862198       1 shared_informer.go:357] "Caches are synced" controller="job"
I0729 21:32:42.871841       1 shared_informer.go:357] "Caches are synced" controller="cronjob"
I0729 21:32:42.872202       1 shared_informer.go:357] "Caches are synced" controller="ReplicaSet"
I0729 21:32:42.872879       1 shared_informer.go:357] "Caches are synced" controller="TTL"
I0729 21:32:42.873068       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0729 21:32:42.873248       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0729 21:32:42.875995       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0729 21:32:42.876114       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0729 21:32:42.876132       1 shared_informer.go:350] "Waiting for caches to sync" controller="cidrallocator"
I0729 21:32:42.876145       1 shared_informer.go:357] "Caches are synced" controller="cidrallocator"
I0729 21:32:42.873358       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0729 21:32:42.873383       1 shared_informer.go:357] "Caches are synced" controller="expand"
I0729 21:32:42.873392       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0729 21:32:42.873434       1 shared_informer.go:357] "Caches are synced" controller="stateful set"
I0729 21:32:42.876417       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0729 21:32:42.877803       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0729 21:32:42.876520       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0729 21:32:42.873346       1 shared_informer.go:357] "Caches are synced" controller="crt configmap"
I0729 21:32:42.941955       1 shared_informer.go:357] "Caches are synced" controller="deployment"
I0729 21:32:42.949984       1 shared_informer.go:357] "Caches are synced" controller="disruption"
I0729 21:32:42.950890       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0729 21:32:42.951112       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0729 21:32:42.951193       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0729 21:32:42.952502       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0729 21:32:42.955797       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0729 21:32:42.962843       1 shared_informer.go:357] "Caches are synced" controller="PV protection"
I0729 21:32:42.962846       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0729 21:32:42.962889       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0729 21:32:43.041246       1 shared_informer.go:357] "Caches are synced" controller="validatingadmissionpolicy-status"
I0729 21:32:43.048328       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0729 21:32:43.051584       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0729 21:32:43.056743       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0729 21:32:43.459537       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0729 21:32:43.470240       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0729 21:32:43.470316       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0729 21:32:43.470397       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"


==> kube-proxy [0c973f6e730f] <==
I0729 21:32:44.162469       1 server_linux.go:63] "Using iptables proxy"
I0729 21:32:44.767136       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0729 21:32:44.768074       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0729 21:32:44.904422       1 server.go:254] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0729 21:32:44.904908       1 server_linux.go:145] "Using iptables Proxier"
I0729 21:32:44.940127       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0729 21:32:44.945553       1 server.go:516] "Version info" version="v1.33.1"
I0729 21:32:44.945708       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0729 21:32:44.970248       1 config.go:199] "Starting service config controller"
I0729 21:32:44.970299       1 config.go:105] "Starting endpoint slice config controller"
I0729 21:32:44.979878       1 config.go:440] "Starting serviceCIDR config controller"
I0729 21:32:44.988719       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0729 21:32:44.988718       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0729 21:32:44.988836       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0729 21:32:44.988857       1 config.go:329] "Starting node config controller"
I0729 21:32:44.988871       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0729 21:32:45.139830       1 shared_informer.go:357] "Caches are synced" controller="service config"
I0729 21:32:45.139977       1 shared_informer.go:357] "Caches are synced" controller="node config"
I0729 21:32:45.140016       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"
I0729 21:32:45.140038       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"


==> kube-proxy [f97a9be228b9] <==
I0802 19:12:41.053108       1 server_linux.go:63] "Using iptables proxy"
I0802 19:12:42.034423       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0802 19:12:42.041211       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0802 19:12:42.742539       1 server.go:254] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0802 19:12:42.742727       1 server_linux.go:145] "Using iptables Proxier"
I0802 19:12:42.772405       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0802 19:12:42.779648       1 server.go:516] "Version info" version="v1.33.1"
I0802 19:12:42.779734       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0802 19:12:42.843130       1 config.go:199] "Starting service config controller"
I0802 19:12:42.843489       1 config.go:105] "Starting endpoint slice config controller"
I0802 19:12:42.843905       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0802 19:12:42.844081       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0802 19:12:42.851118       1 config.go:440] "Starting serviceCIDR config controller"
I0802 19:12:42.851199       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0802 19:12:42.851275       1 config.go:329] "Starting node config controller"
I0802 19:12:42.851297       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0802 19:12:42.944216       1 shared_informer.go:357] "Caches are synced" controller="service config"
I0802 19:12:42.948238       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"
I0802 19:12:42.951752       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"
I0802 19:12:42.951894       1 shared_informer.go:357] "Caches are synced" controller="node config"


==> kube-scheduler [2005ba0aefe5] <==
I0802 19:12:21.743757       1 serving.go:386] Generated self-signed cert in-memory
W0802 19:12:28.351704       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0802 19:12:28.351818       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0802 19:12:28.351843       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0802 19:12:28.351866       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0802 19:12:28.648257       1 server.go:171] "Starting Kubernetes Scheduler" version="v1.33.1"
I0802 19:12:28.648363       1 server.go:173] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0802 19:12:28.667472       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0802 19:12:28.667575       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0802 19:12:28.669367       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0802 19:12:28.669464       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0802 19:12:28.847679       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kube-scheduler [902f2f216d40] <==
I0729 21:32:24.919418       1 serving.go:386] Generated self-signed cert in-memory
W0729 21:32:32.702754       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0729 21:32:32.703360       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0729 21:32:32.703403       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0729 21:32:32.703421       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0729 21:32:32.958307       1 server.go:171] "Starting Kubernetes Scheduler" version="v1.33.1"
I0729 21:32:32.958460       1 server.go:173] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0729 21:32:33.049933       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0729 21:32:33.055998       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0729 21:32:33.057067       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0729 21:32:33.065945       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0729 21:32:33.345918       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0729 22:06:38.144555       1 configmap_cafile_content.go:226] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0729 22:06:38.144229       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
I0729 22:06:38.146663       1 secure_serving.go:259] Stopped listening on 127.0.0.1:10259
E0729 22:06:38.236946       1 run.go:72] "command failed" err="finished without leader elect"


==> kubelet <==
Aug 02 19:12:16 minikube kubelet[1631]: E0802 19:12:16.155740    1631 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Aug 02 19:12:16 minikube kubelet[1631]: I0802 19:12:16.556160    1631 kubelet_node_status.go:75] "Attempting to register node" node="minikube"
Aug 02 19:12:16 minikube kubelet[1631]: E0802 19:12:16.559310    1631 kubelet_node_status.go:107] "Unable to register node with API server" err="Post \"https://192.168.49.2:8443/api/v1/nodes\": dial tcp 192.168.49.2:8443: connect: connection refused" node="minikube"
Aug 02 19:12:16 minikube kubelet[1631]: E0802 19:12:16.884354    1631 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://192.168.49.2:8443/api/v1/namespaces/default/events\": dial tcp 192.168.49.2:8443: connect: connection refused" event="&Event{ObjectMeta:{minikube.185808fb340f9153  default    0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},InvolvedObject:ObjectReference{Kind:Node,Namespace:,Name:minikube,UID:minikube,APIVersion:,ResourceVersion:,FieldPath:,},Reason:Starting,Message:Starting kubelet.,Source:EventSource{Component:kubelet,Host:minikube,},FirstTimestamp:2025-08-02 19:12:09.864057171 +0000 UTC m=+0.558342635,LastTimestamp:2025-08-02 19:12:09.864057171 +0000 UTC m=+0.558342635,Count:1,Type:Normal,EventTime:0001-01-01 00:00:00 +0000 UTC,Series:nil,Action:,Related:nil,ReportingController:kubelet,ReportingInstance:minikube,}"
Aug 02 19:12:17 minikube kubelet[1631]: E0802 19:12:17.274115    1631 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Aug 02 19:12:17 minikube kubelet[1631]: E0802 19:12:17.363647    1631 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Aug 02 19:12:17 minikube kubelet[1631]: E0802 19:12:17.446375    1631 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Aug 02 19:12:17 minikube kubelet[1631]: E0802 19:12:17.460006    1631 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Aug 02 19:12:18 minikube kubelet[1631]: E0802 19:12:18.157547    1631 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: Get \"https://192.168.49.2:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
Aug 02 19:12:18 minikube kubelet[1631]: E0802 19:12:18.479980    1631 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Aug 02 19:12:18 minikube kubelet[1631]: E0802 19:12:18.480248    1631 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Aug 02 19:12:18 minikube kubelet[1631]: E0802 19:12:18.544504    1631 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Aug 02 19:12:18 minikube kubelet[1631]: E0802 19:12:18.544547    1631 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Aug 02 19:12:19 minikube kubelet[1631]: E0802 19:12:19.143741    1631 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: Get \"https://192.168.49.2:8443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
Aug 02 19:12:19 minikube kubelet[1631]: E0802 19:12:19.145972    1631 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
Aug 02 19:12:19 minikube kubelet[1631]: E0802 19:12:19.544451    1631 reflector.go:200] "Failed to watch" err="failed to list *v1.RuntimeClass: Get \"https://192.168.49.2:8443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.RuntimeClass"
Aug 02 19:12:19 minikube kubelet[1631]: E0802 19:12:19.563730    1631 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Aug 02 19:12:19 minikube kubelet[1631]: E0802 19:12:19.564055    1631 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Aug 02 19:12:20 minikube kubelet[1631]: E0802 19:12:20.158137    1631 eviction_manager.go:292] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"minikube\" not found"
Aug 02 19:12:22 minikube kubelet[1631]: I0802 19:12:22.962320    1631 kubelet_node_status.go:75] "Attempting to register node" node="minikube"
Aug 02 19:12:23 minikube kubelet[1631]: E0802 19:12:23.356060    1631 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Aug 02 19:12:23 minikube kubelet[1631]: E0802 19:12:23.848309    1631 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Aug 02 19:12:24 minikube kubelet[1631]: E0802 19:12:24.651231    1631 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Aug 02 19:12:24 minikube kubelet[1631]: E0802 19:12:24.809485    1631 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Aug 02 19:12:24 minikube kubelet[1631]: E0802 19:12:24.810182    1631 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Aug 02 19:12:24 minikube kubelet[1631]: E0802 19:12:24.846311    1631 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Aug 02 19:12:26 minikube kubelet[1631]: E0802 19:12:26.352659    1631 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Aug 02 19:12:28 minikube kubelet[1631]: I0802 19:12:28.444789    1631 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Aug 02 19:12:28 minikube kubelet[1631]: E0802 19:12:28.580965    1631 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Aug 02 19:12:28 minikube kubelet[1631]: I0802 19:12:28.581045    1631 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-controller-manager-minikube"
Aug 02 19:12:28 minikube kubelet[1631]: I0802 19:12:28.646812    1631 kubelet_node_status.go:124] "Node was previously registered" node="minikube"
Aug 02 19:12:28 minikube kubelet[1631]: I0802 19:12:28.653274    1631 kubelet_node_status.go:78] "Successfully registered node" node="minikube"
Aug 02 19:12:28 minikube kubelet[1631]: I0802 19:12:28.653369    1631 kuberuntime_manager.go:1746] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Aug 02 19:12:28 minikube kubelet[1631]: I0802 19:12:28.657589    1631 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Aug 02 19:12:28 minikube kubelet[1631]: E0802 19:12:28.845436    1631 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"kube-controller-manager-minikube\" already exists" pod="kube-system/kube-controller-manager-minikube"
Aug 02 19:12:28 minikube kubelet[1631]: I0802 19:12:28.845506    1631 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Aug 02 19:12:28 minikube kubelet[1631]: I0802 19:12:28.846821    1631 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Aug 02 19:12:28 minikube kubelet[1631]: I0802 19:12:28.859710    1631 apiserver.go:52] "Watching apiserver"
Aug 02 19:12:29 minikube kubelet[1631]: I0802 19:12:29.047423    1631 desired_state_of_world_populator.go:158] "Finished populating initial desired state of world"
Aug 02 19:12:29 minikube kubelet[1631]: I0802 19:12:29.065454    1631 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/f4f23e45-43db-4c8e-af91-3383849b9aff-lib-modules\") pod \"kube-proxy-p84w6\" (UID: \"f4f23e45-43db-4c8e-af91-3383849b9aff\") " pod="kube-system/kube-proxy-p84w6"
Aug 02 19:12:29 minikube kubelet[1631]: I0802 19:12:29.065688    1631 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/f4f23e45-43db-4c8e-af91-3383849b9aff-xtables-lock\") pod \"kube-proxy-p84w6\" (UID: \"f4f23e45-43db-4c8e-af91-3383849b9aff\") " pod="kube-system/kube-proxy-p84w6"
Aug 02 19:12:29 minikube kubelet[1631]: I0802 19:12:29.065787    1631 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/54e91ec2-b11d-4501-a7d4-6465c0b74a54-tmp\") pod \"storage-provisioner\" (UID: \"54e91ec2-b11d-4501-a7d4-6465c0b74a54\") " pod="kube-system/storage-provisioner"
Aug 02 19:12:29 minikube kubelet[1631]: E0802 19:12:29.257524    1631 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Aug 02 19:12:29 minikube kubelet[1631]: I0802 19:12:29.257659    1631 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Aug 02 19:12:29 minikube kubelet[1631]: E0802 19:12:29.354633    1631 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Aug 02 19:12:29 minikube kubelet[1631]: E0802 19:12:29.642239    1631 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Aug 02 19:12:34 minikube kubelet[1631]: I0802 19:12:34.456913    1631 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="1872b20842456ff792021d7ca5e2883d266f2f0029263a091270174cf03b118b"
Aug 02 19:12:34 minikube kubelet[1631]: I0802 19:12:34.635675    1631 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="25893c59c0ad503fd4b1d9ad6aabb5ef19f7eccb4e64b889282d46c7be1e32d9"
Aug 02 19:12:58 minikube kubelet[1631]: I0802 19:12:58.904872    1631 scope.go:117] "RemoveContainer" containerID="61d659ca92e18f7d2f619c7e841898f1c5e8340e150bd34820616f8cd5b60e2c"
Aug 02 19:12:58 minikube kubelet[1631]: I0802 19:12:58.905634    1631 scope.go:117] "RemoveContainer" containerID="ed5cd255bd9f618c9389e7ebd1a8e8ae0bb3cc668063f8b1d5de269294fd558e"
Aug 02 19:12:58 minikube kubelet[1631]: E0802 19:12:58.906180    1631 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(54e91ec2-b11d-4501-a7d4-6465c0b74a54)\"" pod="kube-system/storage-provisioner" podUID="54e91ec2-b11d-4501-a7d4-6465c0b74a54"
Aug 02 19:13:13 minikube kubelet[1631]: I0802 19:13:13.820985    1631 scope.go:117] "RemoveContainer" containerID="ed5cd255bd9f618c9389e7ebd1a8e8ae0bb3cc668063f8b1d5de269294fd558e"
Aug 02 19:40:12 minikube kubelet[1631]: I0802 19:40:12.593672    1631 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-wrvqp\" (UniqueName: \"kubernetes.io/projected/bfd06831-112d-43b2-a632-d3dca4b1cad2-kube-api-access-wrvqp\") pod \"nginx-deployment-647677fc66-zb46f\" (UID: \"bfd06831-112d-43b2-a632-d3dca4b1cad2\") " pod="default/nginx-deployment-647677fc66-zb46f"
Aug 02 19:40:15 minikube kubelet[1631]: I0802 19:40:12.783935    1631 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-cpgs8\" (UniqueName: \"kubernetes.io/projected/0a1c1927-f211-4129-ac9a-a314180f1d2c-kube-api-access-cpgs8\") pod \"nginx-deployment-647677fc66-rdrp4\" (UID: \"0a1c1927-f211-4129-ac9a-a314180f1d2c\") " pod="default/nginx-deployment-647677fc66-rdrp4"
Aug 02 19:40:20 minikube kubelet[1631]: E0802 19:40:20.010128    1631 kubelet.go:2627] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.001s"
Aug 02 19:40:36 minikube kubelet[1631]: I0802 19:40:36.690283    1631 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="6b408f80bd764aecfb727b0f2dd5de50db428e250bbf21ff8ceaa561454cc98f"
Aug 02 19:40:37 minikube kubelet[1631]: I0802 19:40:37.545402    1631 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="727a70a08d7be4da9528e453459ebb9e51962f384b7f2767f9918827c359074c"
Aug 02 19:40:54 minikube kubelet[1631]: I0802 19:40:54.365307    1631 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/nginx-deployment-647677fc66-rdrp4" podStartSLOduration=45.260496997 podStartE2EDuration="45.260496997s" podCreationTimestamp="2025-08-02 19:40:09 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-08-02 19:40:54.166138603 +0000 UTC m=+1657.364079681" watchObservedRunningTime="2025-08-02 19:40:54.260496997 +0000 UTC m=+1657.458437980"
Aug 02 19:40:55 minikube kubelet[1631]: I0802 19:40:55.048030    1631 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/nginx-deployment-647677fc66-zb46f" podStartSLOduration=46.047984052 podStartE2EDuration="46.047984052s" podCreationTimestamp="2025-08-02 19:40:09 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-08-02 19:40:54.863246785 +0000 UTC m=+1658.061187767" watchObservedRunningTime="2025-08-02 19:40:55.047984052 +0000 UTC m=+1658.245925035"
Aug 02 19:43:18 minikube kubelet[1631]: E0802 19:43:18.201076    1631 kubelet.go:2627] "Housekeeping took longer than expected" err="housekeeping took too long" expected="1s" actual="1.37s"


==> storage-provisioner [0ff83287cd50] <==
W0802 19:53:01.728089       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:01.745201       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:03.753382       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:03.766445       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:05.777606       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:05.798372       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:07.805784       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:07.820962       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:09.835149       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:09.866295       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:11.882092       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:11.907854       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:14.436931       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:14.454820       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:17.495354       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:17.516060       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:19.530704       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:19.550793       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:21.558025       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:21.575196       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:23.584782       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:23.599435       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:25.607666       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:25.629279       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:27.646123       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:27.661771       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:29.702541       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:29.740433       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:31.748477       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:31.764931       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:33.772086       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:33.805830       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:35.814917       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:35.834366       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:37.843165       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:37.861975       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:39.923167       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:40.147486       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:42.308884       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:42.685033       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:45.979050       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:47.133434       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:49.178984       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:49.219742       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:51.245049       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:51.333723       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:53.364064       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:53.434638       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:55.448192       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:55.520029       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:57.548599       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:57.730473       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:59.761641       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:53:59.851454       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:54:01.881885       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:54:01.971306       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:54:04.036917       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:54:04.132736       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:54:06.221467       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0802 19:54:06.277403       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice


==> storage-provisioner [ed5cd255bd9f] <==
I0802 19:12:36.963505       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0802 19:12:57.715039       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused

